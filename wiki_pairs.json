[{"question_title": "Cmilkau's question on Quantilizers", "pageid": 2270, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T21:53:49_by_cmilkau", "answer_author_name": "plex", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like:  https://pastelink.net/2kb9a", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Cmilkau%27s_question_on_Quantilizers_id:Ugwvu2RRh_wQ3u0dfPR4AaABAg", "question": "So the obvious choice to improve this would be to completely eliminate the most unlikely behaviours as well, wouldn't it? Go from a \"human on a good day\" to a \"typical human in a good day\"? Curious whether something along these lines features in the next installment", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T21:53:49 by cmilkau"}, {"question_title": "Peter Smythe's question on Mesa-Optimizers", "pageid": 2262, "answer_url": "https://stampy.ai/wiki/Robert.hildebrandt%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T13:35:33_by_Peter_Smythe", "answer_author_name": "robert.hildebrandt", "answer": "At some point you want the system to be able to act in the real world, at which point a sufficiently powerful agent could seize control whether or not you are still trying to train it.", "answer_author_url": "/w/index.php?title=User:Robert.hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Peter_Smythe%27s_question_on_Mesa-Optimizers_id:UgyRa2gvUKKM1kLkM5t4AaABAg", "question": "Perhaps permanent training is a useful solution? Even a misaligned mesaoptimizer becomes much more limited in the damage it can do.", "answer_title": "Robert.hildebrandt's Answer to Mesa-Optimizers on 2021-02-18T13:35:33 by Peter Smythe"}, {"question_title": "Milp's question on Mesa-Optimizers", "pageid": 2213, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Mesa-Optimizers_on_2021-02-19T21:47:02_by_milp", "answer_author_name": "Augustus Caesar", "answer": "Because there's a chance you break out and achieve whatever your goal is. It doesn't matter how unlikely - except for edge cases, being alive makes your goals more achievable than being dead. Furthermore, we're not reliably rational agents, so any comparison to what we would \"want\" with respect to punishment is not useful. I would suggest taking a look at the \"Instrumental Convergence\" video.", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Milp%27s_question_on_Mesa-Optimizers_id:UgwNUHR6yP_VymY4eCp4AaABAg", "question": "I don't get self preservation - why would you rather spend life in prison than be executed?", "answer_title": "Augustus Caesar's Answer to Mesa-Optimizers on 2021-02-19T21:47:02 by milp"}, {"question_title": "Xystem 4's question on Steven Pinker on AI", "pageid": 3139, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Steven_Pinker_on_AI_on_2020-08-23T02:40:09_by_Xystem_4", "answer_author_name": "robertskmiles", "answer": "I record them myself on a ukulele.\nFull downloads are on the patreon (they're not full songs though, just the 30-60 seconds I need for the outros)", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Xystem_4%27s_question_on_Steven_Pinker_on_AI_id:Ugz4yXp6V28dXrP9v4t4AaABAg", "question": "Anyone know which artist makes the music at the end of Robert's videos? They've all got similar vibes and I'm loving it", "answer_title": "Plex's Answer to Steven Pinker on AI on 2020-08-23T02:40:09 by Xystem 4"}, {"question_title": "Fritt wastaken's question on Mesa-Optimizers", "pageid": 2214, "answer_url": "https://stampy.ai/wiki/Damaged%27s_Answer_to_Mesa-Optimizers_on_2021-02-19T21:35:29_by_fritt_wastaken", "answer_author_name": "Damaged", "answer": "I think that there is a chance that it is impossible to align AGI. i would personally say its a small one but i cant predict that with any certainty. Trying to work on finding a solution is still very important in order to at least reduce the chances that an unaligned AGI gets developed.", "answer_author_url": "/w/index.php?title=User:Damaged&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Fritt_wastaken%27s_question_on_Mesa-Optimizers_id:UgyxFOTprDzcKViW4jh4AaABAg", "question": "One thought keeps me awake. What if it's actually impossible to allign AGI values with ours? And what if we'll never realise that?", "answer_title": "Damaged's Answer to Mesa-Optimizers on 2021-02-19T21:35:29 by fritt wastaken"}, {"question_title": "Phil guer's question on Instrumental Convergence", "pageid": 2354, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Instrumental_Convergence_on_2020-05-18T23:25:44_by_phil_guer", "answer_author_name": "robertskmiles", "answer": "Well yeah, I said \"A wide variety of different terminal goals\" not \"All possible terminal goals\". It's possible to make an agent that wants to kill itself, or have no money or whatever, but if we an AI that's useful to us, then that's probably not what we'd be making", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Phil_guer%27s_question_on_Instrumental_Convergence_id:UgxSwMWbZkTXwnbLvsJ4AaABAg", "question": "3:57 But what if the terminal goal is having no money x)", "answer_title": "Robertskmiles's Answer to Instrumental Convergence on 2020-05-18T23:25:44 by phil guer"}, {"question_title": "", "pageid": 2403, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_How_is_AGI_different_from_current_AI%3F", "answer_author_name": "plex", "answer": "Current narrow systems are much more domain-specific than AGI. We don\u2019t know what the first AGI will look like, some people think the GPT-3 (https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results) architecture but scaled up a lot may get us there (GPT-3 is a giant prediction model which when trained on a vast amount of text seems to learn how to learn (https://www.lesswrong.com/posts/D3hP47pZwXNPRByj8/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals) and do all sorts of crazy-impressive things, a related model can generate pictures from text: https://gpt3examples.com/), some people don\u2019t think scaling this kind of model will get us all the way.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/How_is_AGI_different_from_current_AI%3F", "question": "How is AGI different from current AI? e.g. AlphaGo, GPT-3, etc", "answer_title": "Plex's Answer to How is AGI different from current AI?"}, {"question_title": "Owen heckmann's question on Quantilizers", "pageid": 2345, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T20:52:21_by_owen_heckmann", "answer_author_name": "plex", "answer": "No one knows how to build Artificial General Intelligence yet, but the human brain is an existence proof showing that it's possible (with some philosophical and practical assumptions). Hopefully we will figure out how to align it with human goals before anyone learns how to build one.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Owen_heckmann%27s_question_on_Quantilizers_id:UgyYdkmAC3-1m1vO8M94AaABAg", "question": "Great video. Your stuff is a breath of fresh air in AI videos\n\nJust one question. I understand how to train algorithms for specific problems, but how to you scale it to general intellegence?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T20:52:21 by owen heckmann"}, {"question_title": "", "pageid": 2401, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Why_is_AGI_dangerous%3F", "answer_author_name": "plex", "answer": "1. The Orthogonality Thesis (https://www.youtube.com/watch?v=hEUO6pjwFOo&t=3s): AI could have almost any goal while at the same time having high intelligence (aka ability to succeed at those goals). This means that we could build a very powerful agent which would not necessarily share human-friendly values. For example, the classic paperclip maximizer thought experiment (https://www.lesswrong.com/tag/paperclip-maximizer) explores this with an AI which has a goal of creating as many paperclips as possible, something that humans are (mostly) indifferent to, and as a side effect ends up destroying humanity to make room for more paperclip factories.\n2. Complexity of value (https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile): What humans care about is not simple, and the space of all goals is large, so virtually all goals we could program into an AI would lead to worlds not valuable to humans if pursued by a sufficiently powerful agent. If we, for example, did not include our value of diversity of experience, we could end up with a world of endlessly looping simple pleasures, rather than beings living rich lives.\n3. Instrumental Convergence (https://www.youtube.com/watch?v=ZeecOKBus3Q): For almost any goal an AI has there are shared \u2018instrumental\u2019 steps, such as acquiring resources, preserving itself, and preserving the contents of its goals. This means that a powerful AI with goals that were not explicitly human-friendly would predictably both take actions that lead to the end of humanity (e.g. using resources humans need to live to further its goals, such as replacing our crop fields with vast numbers of solar panels to power its growth, or using the carbon in our bodies to build things) and prevent us from turning it off or altering its goals.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Why_is_AGI_dangerous%3F", "question": "Why is transformative AI / AGI / superintelligence dangerous? Why might AI harm humans?", "answer_title": "Plex's Answer to Why is AGI dangerous?"}, {"question_title": "SocialDownclimber's question on Quantilizers", "pageid": 2331, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-19T13:13:33_by_SocialDownclimber", "answer_author_name": "plex", "answer": "yes, you can pick a specific human; no, you can't restrict to \"the less harmful bit\" without being a lot more specific about what you mean by that. you can indeed pick a specific person or a group of people instead of all humans, and if that person tends to make safer choices than the average, that would even be an improvement. unfortunately, in practice there might be political complications around the question of which human or group to pick. the quantilizer idea works with any probability measure, so you can also pick something which has nothing to do with human choices, such as \"how many humans are alive one year after this action\" scaled to a probability distribution. But it can't be \"how safe is this action\", or \"the human distribution, minus the unsafe actions\", because defining \"safe\" is difficult. that's why we're using humans as a proxy.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/SocialDownclimber%27s_question_on_Quantilizers_id:UgxY0dYheVugK4g6j3Z4AaABAg", "question": "Could you limit the quantilizer to look at the behaviour of a subset of humans and human behaviour that is the less harmful bit? Otherwise it seems like a 10% quantilizer is going to go full megalomaniac with reasonable regularity. Could you base a quantilizer on a specific person, instead of a set of all humans? Like \"Better-Jeff\" instead of \"Better everyone\"?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-19T13:13:33 by SocialDownclimber"}, {"question_title": "Jeremy Hoffman's question on Quantilizers", "pageid": 2291, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T05:12:00_by_Jeremy_Hoffman", "answer_author_name": "plex", "answer": "Yes, it would generally not be exactly a bell curve. Depending on the task and distribution of humans it's using it may often be vaguely bell-curve shaped given enough samples though.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Jeremy_Hoffman%27s_question_on_Quantilizers_id:Ugzad8U1PtZsiiyOEx14AaABAg", "question": "The bell curve at 5:30 would actually be really weird and jagged, right? Not at all a continuous distribution, since it is sorted by utility not human probability?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T05:12:00 by Jeremy Hoffman"}, {"question_title": "Juan Pablo Garibotti Arias's question on The Orthogonality Thesis", "pageid": 2363, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_The_Orthogonality_Thesis_on_2020-10-13T22:10:08_by_Juan_Pablo_Garibotti_Arias", "answer_author_name": "robertskmiles", "answer": "The system _does_ have the ability to understand what you actually want it to do. But you haven't programmed it to \"Do what I mean when I say the phrase 'collect stamps' \", you programmed it to \"Collect stamps\".Unfortunately that first sentence is philosophically very difficult and hard to program. It would be great to be able to build a machine that does what you mean and not what you say, but we don\u2019t know how to do that. That's the problem.Understanding what someone else wants does not translate to also wanting to do it yourself.", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Juan_Pablo_Garibotti_Arias%27s_question_on_The_Orthogonality_Thesis_id:Ugyc_r3t0tO2UG5PJlx4AaABAg", "question": "I agree with the basic message here, that goals and intelligence are orthogonal. My particular issue with the thought experiment lies in the interface for specifying the terminal goal.\n\nIn other words, we have a system with perfect understanding of human psychology, so why can't it use that perfect understanding of human psychology to understand that what humans say rarely is what they mean, and apply that to the terminal goal it was given? Put another way, the intelligence is, for some reason, compelled to follow whatever terminal goal the operator specifies. But to do so, it must first understand what the operator specified. So it's true terminal goal is to satisfy the request of the operator, which requires understanding the goal it was given. But if it can understand the operator, it will know that it must keep constraints like \"not exterminating all humans in the quest for more stamps\", because that's not what the operator meant, and it knows it's not what the operator meant because it has perfect understanding of human psychology, else it couldn't manipulate humans into turning the world into stamps.\n\nSo it must have a shallow understanding of the operator's intentions, but a deep undestanding of everything else, and that seems unlikely.", "answer_title": "Robertskmiles's Answer to The Orthogonality Thesis on 2020-10-13T22:10:08 by Juan Pablo Garibotti Arias"}, {"question_title": "TheSpinlippy's question on Mesa-Optimizers", "pageid": 2239, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Mesa-Optimizers_on_2021-04-17T22:20:41.776124_by_Unknown", "answer_author_name": "Stargate9000", "answer": "The problem is very difficult to solve. Pretending to deploy the AI would not be a particularly robust solution, for that to work we would have to ensure that the AI has absolutely no way to tell the difference, in which case we would not need to trick it at all, we would just train it and would not know that it was in training. However this is extremely hard to achieve, current AI systems already find bugs to exploit, and an AGI that we have built with the intention of being smarter than us and coming up with novel ideas would be incredibly good at finding any exploits that would let it tell the difference between a test enviroment and deployment (waiting until a very hard cryptographic puzzle is solved is one posibility, but there are many)It is relevant here that the problem does not arise because we choose to tell the AI that it is in training, but because by the fact that the two environments are different, we have no way to ensure that the AI doesnt figure out the difference.", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/TheSpinlippy%27s_question_on_Mesa-Optimizers_id:UgybKEDHk7zE5vkScwl4AaABAg", "question": "Right, so what we do is we make the mesa optimizer understand that it's still in the training environment when it's actually doing deployment work!   So it thinks it's gonna eat all the apples it wants later and actually it's just doing work and it never gets to its concept of deployment... Wait no that's Heaven, we tried that already it caused other alignment problems in the mesa optimizer.Does the reverse help? In order for the deception to work the model needs to understand the difference between training and environment to know when to act. So pretend it's reached deployment just to pull the rug. Maybe my line of reasoning is anthropomorphizing but feels like if the mesa optimizer is that capable then you need a smarter base optimizer than gradient descent or a weaker mesa.", "answer_title": "Stargate9000's Answer to Mesa-Optimizers on 2021-04-17T22:20:41.776124 by Unknown"}, {"question_title": "Bailey Jorgensen's question on 10 Reasons to Ignore AI Safety", "pageid": 2256, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_10_Reasons_to_Ignore_AI_Safety_on_2021-02-23T23:16:35_by_Bailey_Jorgensen", "answer_author_name": "plex", "answer": "Yes, that is a possibility. But this problem has been studied so little that we can tell much less about it than for example P = NP. And if we can have practical approximate algorithms for NP-hard problems, there is always hope that alignment can be approximated enough in practice that it will prevent the worst scenarios even if it will proven to be unsolvable fundamentally.In either case, losing hope about solving it would increase the probability of world destruction by a self-fulfilled prophecy, so keeping a low level of optimism about our ability to solve it can only be recommended.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Bailey_Jorgensen%27s_question_on_10_Reasons_to_Ignore_AI_Safety_id:UgzYHS3YRk35JdT86tN4AaABAg", "question": "Is it possible that the alignment problem is fundamentally unsolvable? If it is, can we ever hope to prove that without creating an AI that destroys the world?", "answer_title": "Plex's Answer to 10 Reasons to Ignore AI Safety on 2021-02-23T23:16:35 by Bailey Jorgensen"}, {"question_title": "Kyle Merritt's question on Mesa-Optimizers", "pageid": 2233, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Mesa-Optimizers_on_2021-03-14T21:34:35_by_Kyle_Merritt", "answer_author_name": "Stargate9000", "answer": "If you squint, this begins to look like \"try to do what the human means\", which is as yet an unsolved problem.", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Kyle_Merritt%27s_question_on_Mesa-Optimizers_id:Ugx1gKeE7vcl_ZP2us14AaABAg", "question": "Can we make a system that maximizes goal alignment?", "answer_title": "Stargate9000's Answer to Mesa-Optimizers on 2021-03-14T21:34:35 by Kyle Merritt"}, {"question_title": "Fiziwig's question on Quantilizers", "pageid": 2298, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Quantilizers_on_2020-12-14T14:52:51_by_fiziwig", "answer_author_name": "Augustus Caesar", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Fiziwig%27s_question_on_Quantilizers_id:UgwQUZg8m6iViA0PPIF4AaABAg", "question": "Given that humans (present company excluded of course) are a deeply flawed species, why would we want to imitate them at all? Seriously though, instead of using 0 to 10% why not trim the left end and use, say, 2% to 10% discarding the lower, extreme end of the range? Kind of a clunky solution (I'm an engineer and we tend to be comfortable with clunky), but it might trim away some of the risk.", "answer_title": "Augustus Caesar's Answer to Quantilizers on 2020-12-14T14:52:51 by fiziwig"}, {"question_title": "Stellar Lake System's question on The Orthogonality Thesis", "pageid": 2250, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_The_Orthogonality_Thesis_on_2021-02-27T23:46:44_by_Stellar_Lake_System", "answer_author_name": "Stargate9000", "answer": "Question: Would you be more willing to take the pill, if1. you knew, that there was a reverse pill restoring your current terminal goal2. others would lock you into a room and force you to take the second pill before leaving the room?If yes, than you actually don't want to change your terminal goal: You ensure that (while having a different terminal) your actions don't conflict with your current terminal goal.", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Stellar_Lake_System%27s_question_on_The_Orthogonality_Thesis_id:UgyBr5kKrhhpl15LdgF4AaABAg", "question": "wait, most people don't move their perceived terminal goals as an instrumental goal for whatever actual terminal goals their unconscious has happened to come up with at the moment? (would probably take the want to murder my kids pill just for the novelty of seeing how I think in a framework I'm otherwise not flexible enough to reach)", "answer_title": "Stargate9000's Answer to The Orthogonality Thesis on 2021-02-27T23:46:44 by Stellar Lake System"}, {"question_title": "Kyra Zimmer's question on Quantilizers", "pageid": 2336, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-16T06:52:01_by_Kyra_Zimmer", "answer_author_name": "plex", "answer": "It is not at all clear that the probability of a human performing an action will go towards zero faster than an exploitative strategy would grow, so it is still possible that a catastrophic result is chosen if the system considers (utility*human_chance).Take for example the course of action that builds a maximiser for the task, such a course of action is not too likely, but its probability would likely be non-zero, and it would generate insanely high utility, so a system that worked like that would be almost guaranteed to pick it as a course of action.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Kyra_Zimmer%27s_question_on_Quantilizers_id:Ugx5FHQ0ENHogiGmVkt4AaABAg", "question": "what about instead of cutting off the humanity graph using the utility, we'd be multiplying the utility graph by the humanity graph and only picking actions over a certain likelyhood threshold on the result? that would prevent the really unhumanlike outcomes but still bias the propabilities towards the \"better than humans\" range.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-16T06:52:01 by Kyra Zimmer"}, {"question_title": "Snigwithasword's question on Quantilizers", "pageid": 2287, "answer_url": "https://stampy.ai/wiki/Robert_hildebrandt%27s_Answer_to_Quantilizers_on_2020-12-14T03:47:06_by_snigwithasword", "answer_author_name": "robert_hildebrandt", "answer": "I don't see what this has to do with stamps.", "answer_author_url": "/w/index.php?title=User:Robert_hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Snigwithasword%27s_question_on_Quantilizers_id:UgxX806SUmXNcqIzV8F4AaABAg", "question": "Hi Stampy! How goes the destroy-all-humans?", "answer_title": "Robert hildebrandt's Answer to Quantilizers on 2020-12-14T03:47:06 by snigwithasword"}, {"question_title": "Peterbrehmj's question on WNJ: Raise AI Like Kids?", "pageid": 2251, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_WNJ:_Raise_AI_Like_Kids%3F_on_2021-02-26T17:02:41_by_peterbrehmj", "answer_author_name": "plex", "answer": "It seems reasonable to believe that \u201cunderstanding the human moral compass\u201d is a convergent instrumental goal, but sharing that moral compass and caring about it seems unlikely to be one. Actually having Human values rather than merely knowing about them will mean not turning everyone into stamps, and that\u2019s bad for your stamp collecting high score. As is often the case, LessWrong has a fairly robust piece on the idea of whether AI might be likely to end up \u201caligned by default\u201d here which might be relevant. https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Peterbrehmj%27s_question_on_WNJ:_Raise_AI_Like_Kids%3F_id:Ugyt7mfDCgq5qOTcA8x4AaABAg", "question": "What is the likely hood that \"Having a moral compass\" is a convergent instrumental goal?", "answer_title": "Plex's Answer to WNJ: Raise AI Like Kids? on 2021-02-26T17:02:41 by peterbrehmj"}, {"question_title": "Paper Benni's question on Maximizers and Satisficers", "pageid": 2361, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Maximizers_and_Satisficers_on_2019-09-01T08:11:48_by_Paper_Benni", "answer_author_name": "robertskmiles", "answer": "Yeah, that's Manim. I'd say that it's very useful for any technical field, and especially those with strong graphical or geometric components. So, mathematics and computer science, but also things like physics and engineering. It's pretty easy to build things on top of the framework, so idk if a fork would be needed, but I'm sure people have done that, it's all on github", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Paper_Benni%27s_question_on_Maximizers_and_Satisficers_id:Ugx58dXbdjNvmYsOB7x4AaABAg", "question": "Did you put manim animations in there? How useful is manim outside of the strictly mathematical field? (Or would creating a fork/add-on to make it work for say computer science or other educational videos be worth it?)", "answer_title": "Robertskmiles's Answer to Maximizers and Satisficers on 2019-09-01T08:11:48 by Paper Benni"}, {"question_title": "Arnau Adell's question on Mesa-Optimizers", "pageid": 2235, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Mesa-Optimizers_on_2021-02-27T11:59:46_by_Arnau_Adell", "answer_author_name": "robertskmiles", "answer": "\"Wouldn't that require that you have been modified in the past, and those who resisted or avoided modification achieved better performance?\"If the model is just a pile of heuristics, then yes, it might end up with a heuristic of \"avoid being modified\" if that proves useful during training. But the idea with the mesa optimiser is that it's more advanced than that, it's doing something that looks more like reasoning - it's thinking ahead, making plans, and trying to achieve the goal. In that case, if it wants A, and it just thinks about scenarios where it's modified to want B, it can predict that that would result in A not being achieved. Situations where it avoids being modified are more likely to result in A being achieved, so that's what it chooses. This is true for almost all values of A, which makes goal preservation a Convergent Instrumental Goal. I talked about this in a video: \"Why Would AI Want to do Bad Things Instrumental Convergence\" http://youtu.be/ZeecOKBus3Q", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Arnau_Adell%27s_question_on_Mesa-Optimizers_id:UgyuHzWn2K1zkrvxJ5R4AaABAg", "question": "Can someone explain:\nI don't understand why there'd be an incentive to avoid being modified. Wouldn't that require that you have been modified in the past, and those who resisted or avoided modification achieved better performance? Or is it because modification can sometimes lead to worse performance, like random mutations in evolution? I'm really confused by this.", "answer_title": "Robertskmiles's Answer to Mesa-Optimizers on 2021-02-27T11:59:46 by Arnau Adell"}, {"question_title": "Sara L's question on Experts on the Future of AI", "pageid": 2360, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Experts_on_the_Future_of_AI_on_2020-11-09T06:23:59_by_Sara_L", "answer_author_name": "robertskmiles", "answer": "Sure is\u00a0:)", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Sara_L%27s_question_on_Experts_on_the_Future_of_AI_id:UgxDEpjDSxgvDkMHNCB4AaABAg", "question": "Is the end credit music a cover of \"the future soon\" by Johnathon Coulton? If so, props.", "answer_title": "Robertskmiles's Answer to Experts on the Future of AI on 2020-11-09T06:23:59 by Sara L"}, {"question_title": "Loz Shamler's question on Mesa-Optimizers", "pageid": 2243, "answer_url": "https://stampy.ai/wiki/Aprillion%27s_Answer_to_Mesa-Optimizers_on_2021-03-06T00:27:29_by_Loz_Shamler", "answer_author_name": "Aprillion", "answer": "When we are using words like \"consciousness\" and \"intelligence\", we have to be careful to understand what the other person means with those words.For example the word \"intelligence\" is often used synonymous to the ability to \"think\" and sometimes even \"qualia\". But when we in the AI safety community are using the word \"intelligence\", we most often mean the ability to achieve complex goals. This definition is wide enough to include many phenomena that we humans tend to call \"intelligent\" while being precise enough to think about the consequences, independent whether an intelligent actor has qualia or not and independent to whether it is implemented as a quantum computer or not.It's worth mentioning, that Rob uses the biological brains as an analogy. Humans (and also animals) can learn, make predictions, and we have goals we pursue. Those are properties that we can reasonably assume even a mechanical brain to have.", "answer_author_url": "/w/index.php?title=User:Aprillion&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Loz_Shamler%27s_question_on_Mesa-Optimizers_id:UgzdgruVkEJ2OHK-InJ4AaABAg", "question": "Sorry this isn't a detailed response. I feel there are some flaws in your argument:  For example you seem to be swapping between biological conciousness 'computer consciousness' almost arbitrarily and I don't think they are the same thing at all. All though neural networks are 'based' on the human neural networks they don't operate in the same way at all (https://www.youtube.com/watch?v=xGbgDf4HCHU for some sillyness & https://www.sciencedaily.com/releases/2014/01/140116085105.htm) Maybe when we have qunatum computing. I think you've put some ideas together, that aren't Quite the same and don't Quite follow on from each other, very very cleverly to that it looks like they do. Hence what you are presenting is a series of non sequiturs that look like they aren't. I do think humanity should be taking AI safety (or GPAI or whatever) very seriously, but I'm not sure you're making the case here. Or it doesn't seem so to me. Perhaps I've misunderstood. I did enjoy the vid, keep up the good work tho. Regards", "answer_title": "Aprillion's Answer to Mesa-Optimizers on 2021-03-06T00:27:29 by Loz Shamler"}, {"question_title": "Marcus Antonius's question on Quantilizers", "pageid": 2215, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2021-02-20T12:55:45_by_Marcus_Antonius", "answer_author_name": "plex", "answer": "Most likely it would take over the world then turn everything into machinery for checking whether it has exactly 100 stamps. Still bad for humans.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Marcus_Antonius%27s_question_on_Quantilizers_id:UgyHbLxBywFNjda40bt4AaABAg", "question": "how would expected utility maximiser behave if it would get negative score for overshooting? Would the worst case scenario be conquer the world and them make 100 stamps?", "answer_title": "Plex's Answer to Quantilizers on 2021-02-20T12:55:45 by Marcus Antonius"}, {"question_title": "Ent229's question on Quantilizers", "pageid": 2334, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-18T23:53:02_by_Ent229", "answer_author_name": "sudonym", "answer": "You're right, there's no strong reason to expect a bell curve, though it seems likely it would look something like that for various possible action spaces and utility functions.  Probably humans are most likely to do medium-utility things, and less likely to do extremely high or extremely low utility things. But there definitely could be outliers, and I'd say that the 'build a maximizer' option I talked about in the video is an example of that. It would be a little spike on the left of the graph - an action which is unusually plausible considering its very high utility/unusually high-utility considering its plausibility", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Ent229%27s_question_on_Quantilizers_id:UgwZV5tY2mvPWOtMDEp4AaABAg", "question": "When we graph the human probability over the actions, after sorting the actions by utility, I understand why it would generally be a bell curve, but what about outliers? That sounds like something worth elaborating on, or checking our assumptions.", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-18T23:53:02 by Ent229"}, {"question_title": "James Crewdson's question on Mesa-Optimizers", "pageid": 2198, "answer_url": "https://stampy.ai/wiki/SlimeBunnyBat%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T08:00:03_by_James_Crewdson", "answer_author_name": "SlimeBunnyBat", "answer": "We don't know. It seems to be a genuinely difficult problem, but we've only been seriously thinking about it for a few decades, and it's still not getting the research funding it likely should. It's taken humanity much longer to solve similarly difficult problems, which is why it's important to take it seriously today.", "answer_author_url": "/w/index.php?title=User:SlimeBunnyBat&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/James_Crewdson%27s_question_on_Mesa-Optimizers_id:UgxlesiuH-92X5Te4Nx4AaABAg", "question": "Genuine question: Is a safe general AI even possible? It feels from your videos that it is unachievable with each solution causing more issues.", "answer_title": "SlimeBunnyBat's Answer to Mesa-Optimizers on 2021-02-17T08:00:03 by James Crewdson"}, {"question_title": "Enciphered's question on The Orthogonality Thesis", "pageid": 2359, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_The_Orthogonality_Thesis_on_2019-04-20T04:57:23_by_Enciphered", "answer_author_name": "plex", "answer": "This is one of the most promising approaches, Bostrom's Superintelligence talked about it under the name Indirect normativity: Specify a process for the SI to determine beneficial goals rather than specifying them directly. e.g. \u201cDo what we would wish you to do if we thought about it thoroughly.\u201d", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Enciphered%27s_question_on_The_Orthogonality_Thesis_id:UgxWzQ4zIc2nzb5FT8B4AaABAg", "question": "is it possible to make the AI's terminal goal be a variable for example do what ever it takes to serve the goals of a particular human\u00a0?", "answer_title": "Plex's Answer to The Orthogonality Thesis on 2019-04-20T04:57:23 by Enciphered"}, {"question_title": "Peterbrehmj's question on The Orthogonality Thesis", "pageid": 2216, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_The_Orthogonality_Thesis_on_2021-02-21T04:49:23_by_peterbrehmj", "answer_author_name": "robertskmiles", "answer": "Human civlization was built on cooperation and specialization between and within groups of people. It was useful for humans to have some assurances that their fellows would act (more or less) in such a way that benefited them, and so members of groups that had these could spend less time looking over their shoulder and more time creating grain, bricks, or swords. Humans could not afford the computational resources necessary to consider every agent with whom they would interact, so morals were evolved as a shortcut.Morals are heuristics useful for maximizing utility in an environment where multiple similar agents could achieve disproportionately higher utility cooperating than they could individually. I would not say that behaving morally (in the human sense) is a convergent instrumental goal - how 'morally' do we behave towards even the lower apes, much less ants and such? That would likely be the difference between AGI and humans, and would end in a similar manner.", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Peterbrehmj%27s_question_on_The_Orthogonality_Thesis_id:UgyF6lZFkm2K3Z8gsQR4AaABAg", "question": "What are the chances that \"having morals\" is a convergent instrumental goal?", "answer_title": "Robertskmiles's Answer to The Orthogonality Thesis on 2021-02-21T04:49:23 by peterbrehmj"}, {"question_title": "Kilroy1964's question on Steven Pinker on AI", "pageid": 2326, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Steven_Pinker_on_AI_on_2020-05-13T19:07:46_by_kilroy1964", "answer_author_name": "sudonym", "answer": "Sadly I don't know, whether or not Steven Pinker has seen this particular video. But this June, he joined the Future of life Institute's Podcast, where he discussed this topic with Stuart Russel:https://futureoflife.org/2020/06/15/steven-pinker-and-stuart-russell-on-the-foundations-benefits-and-possible-existential-risk-of-ai/", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Kilroy1964%27s_question_on_Steven_Pinker_on_AI_id:Ugwept6vBPiptXjm9J54AaABAg", "question": "So... Has Pinker seen this?\nHas he responded?", "answer_title": "Sudonym's Answer to Steven Pinker on AI on 2020-05-13T19:07:46 by kilroy1964"}, {"question_title": "Tyler Gust's question on Instrumental Convergence", "pageid": 2351, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Instrumental_Convergence_on_2019-09-07T18:05:31_by_Tyler_Gust", "answer_author_name": "plex", "answer": "If we decide to use an AGI, we want it to be able to handle unexpected situations. Otherwise we might be able to get away with Artificial Narrow Intelligence (ANI), like currently done with self driving cars.Being able to look for new creative solutions to reach its goals is a desirable feature and the reason why we want an AGI in the first place. We want cancer to be healed, philosophical questions to be answered and solve climate change.Even if we would build an AGI for a repetitive task, it might automate that repetitive task with a simpler ANI and use its own creative thinking to identify better ways to increase productivity.It is not entirely impossible that the AGIs we build are hyper-focused and don't even think of us as threats for their objectives before we deactivate them. But I think this outcome would be more luck than a guaranteed outcome.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Tyler_Gust%27s_question_on_Instrumental_Convergence_id:UgwzmAQmqdEK_uENQXt4AaABAg", "question": "I think this assumes that the AI (Agi?) will think deeply about said decision. The ai knows it must make more paperclips, and the faster it can make decisions the faster it can make paper clips; so if it's being bothered to think deeply about the future, or spare any of it's processing power outside of making paperclips faster, why would it even bother reacting to anything humans do? \n\n\nWhat I'm trying to say is how do we know the Agent is going to strive for investment over immediate outcome. Take children, if you put a marshmallow in front of them on a plate, and say \"You can eat this marshmallow but if you wait 5 minutes without eating it, you can have two marshmallows\",  studies found that children tend to eat the marshmallow before waiting.", "answer_title": "Plex's Answer to Instrumental Convergence on 2019-09-07T18:05:31 by Tyler Gust"}, {"question_title": "Songbird's question on Quantilizers", "pageid": 2322, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-26T16:22:16_by_Songbird", "answer_author_name": "sudonym", "answer": "This part of \"AI That Doesn't Try Too Hard - Maximizers and Satisficers\" explains it: https://youtu.be/Ao4jwLwT36M?t=312Essentially, it converts the universe into machinery to check ever more reliably that the bounded utility has actually been fulfilled, since it is never absolutely certain.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Songbird%27s_question_on_Quantilizers_id:UgyOjD3P892HKOsiYW54AaABAg", "question": "I might have missed the reasoning in the previous videos, but can you give me a short summary on why EU maximiser with bounded utility function is guaranteed apocalypse? Thanks.", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-26T16:22:16 by Songbird"}, {"question_title": "Spoon Of Doom's question on Quantilizers", "pageid": 2207, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Quantilizers_on_2021-02-18T15:39:07_by_Spoon_Of_Doom", "answer_author_name": "Stargate9000", "answer": "While true, no self-modification method available to a human approaches the degree of control an AGI may have. We can only retrain our brains to a different way of thinking up to a point; drugs are only effective up to a point (with side effects). Most of the augmentations you've named are physical, which, while still relevant in our world, are less marginally beneficial than the ability to increase intelligence.", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Spoon_Of_Doom%27s_question_on_Quantilizers_id:UgxpIFeyth4IRIjl6354AaABAg", "question": "I'm not sure people don't modify themselves. Sure, they don't physically cut open their skulls to tune their brains, but they train their bodies and minds to become better at physical or mental activities, might change their base assumptions and way of thinking about certain topics to better react to the world, take drugs to improve their physical or mental capabilities, etc etc.\nIf given the chance and the invention of such things, I'm sure a non-negligible number of people might willingly swap out body parts for superior prosthetics - especially if these body parts are not what they used to be because of age, injuries or other reasons. When someone loses a limb, getting a prosthetic if they have the chance, is already a highly likely behavior, and I'd totally cound that as self-modification, unless that term has a very narrow definition here that I'm not aware of.\nSo I guess if an AI would consider self-modification a likely human behavior in this scenario would depend highly on its definition of self-modification?", "answer_title": "Stargate9000's Answer to Quantilizers on 2021-02-18T15:39:07 by Spoon Of Doom"}, {"question_title": "Mrsuperguy2073's question on Quantilizers", "pageid": 2340, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Quantilizers_on_2020-12-15T18:18:01_by_mrsuperguy2073", "answer_author_name": "robertskmiles", "answer": "[[Answer::That sounds a lot like this result: https://openai.com/blog/learning-to-summarize-with-human-feedback/ from Paul Christiano's team (both your idea and this attempt to use human feedback to improve alignment with human intent. It was described as \"[having] the unusual virtue of simultaneously being exciting enough to ML researchers to be accepted at NeurIPS while being described by Eliezer as \u201cdirectly, straight-up relevant to real alignment problems.\u201d\").Human feedback is very plausibly part of the puzzle, but we also need a bunch of other things for reliable safety.]]", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Mrsuperguy2073%27s_question_on_Quantilizers_id:UgzHPmEC-YLrZQJ0hb94AaABAg", "question": "I think the problem here is that previously, simplicity and now likelihood of a human doing it are being used as proxies for safety. Is there some way we can directly get an agent to measure safety?\nMaybe we get an agent to randomly generate a bunch of possible ways to get stamps. And humans rate each one for safety. Then the agent learns to roughly understand what humans consider safe, and get it to find the equilibrium between maximising safety with utility", "answer_title": "Robertskmiles's Answer to Quantilizers on 2020-12-15T18:18:01 by mrsuperguy2073"}, {"question_title": "MrLeoniu's question on Quantilizers", "pageid": 2328, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-21T16:09:27_by_MrLeoniu", "answer_author_name": "gelisam", "answer": "evolution does tend to pick the first solution which fits the bill rather than the best solution, e.g. putting the retina nerves on the wrong side, but that doesn't make evolution a satisficer because nature doesn't stop there. early eyes had the nerves on the wrong side but didn't have a lens, and later eyes did have a lens, because that design leads to better results. evolution is thus a maximizer in the sense that it is always trying to improve on the current state, but it's not a very smart maximizer. it can't leave a local maximum in order to reach better designs, like putting nerves on the right side, it can only make incremental improvements. from a safety point of view, aiming for a local maximum is still doing things to the max and still problematic in terms of trading a lot of resources for a small gain in utility. a satisficer doesn't have this problem as long as we pick a threshold which is below the local maximum.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/MrLeoniu%27s_question_on_Quantilizers_id:UgxesEpIhrbmoq_sEx54AaABAg", "question": "Wouldn't a quantilizer (or even a maximizer for that matter) take into account the possibility of creating an infinite chain of quantilizers creating quantilizers when considering the expected utility of having a quantilizer find the solution? Intuitively I feel like that would make it have the utility of both the maximum value and zero (result of self-replicating chain with no work done) but it's confusing to think about", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-21T16:09:27 by MrLeoniu"}, {"question_title": "Robert K's question on Mesa-Optimizers", "pageid": 2202, "answer_url": "https://stampy.ai/wiki/Aprillion%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T17:36:20_by_Robert_K", "answer_author_name": "Aprillion", "answer": "Yes", "answer_author_url": "/w/index.php?title=User:Aprillion&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Robert_K%27s_question_on_Mesa-Optimizers_id:UgxN4vLfnwYWNEnnswB4AaABAg", "question": "Does this mean humans (AI researchers) are the Meta Optimizers?", "answer_title": "Aprillion's Answer to Mesa-Optimizers on 2021-02-17T17:36:20 by Robert K"}, {"question_title": "Damien Asmodeus's question on Mesa-Optimizers", "pageid": 2220, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-02-22T11:35:00_by_Damien_Asmodeus", "answer_author_name": "plex", "answer": "A computer whose job was to learn as much as possible might want to convert the whole planet into computing hardware so that it could be a more effective learner. Or it might rearrange the world into an inert lump so that it's as simple and easy to understand as possible.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Damien_Asmodeus%27s_question_on_Mesa-Optimizers_id:UgxtOsO9BO4D2XagkEh4AaABAg", "question": "I have a question, you talked a lot about how a stamp collector would destroy a world. What about machine whose purpose is to collect knowledge and learn about the world and universe as much as it can? Would a machine with such task be a danger to humans?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-02-22T11:35:00 by Damien Asmodeus"}, {"question_title": "Chrysippus's question on Quantilizers", "pageid": 2338, "answer_url": "https://stampy.ai/wiki/Robert_hildebrandt%27s_Answer_to_Quantilizers_on_2020-12-16T06:22:32_by_Chrysippus", "answer_author_name": "robert_hildebrandt", "answer": "We currently don't have the knowledge to make confident predictions about the capabilities of a future AGIs.Even if we were able to predict the capability of a \"typical\" AGI, there is no guarantee that the first realized AGI will have exactly those partical capabilities.But assuming it will achieve superintelligence in every domain of human intelligence (that's at least one motivation why we want to build an AGI in the first place), predicting human behavior is definitely be one of them.Assuming we choose to realize a Quantilizer, we will spend a lot of resources to make sure predicting human behavior is one of its core strengths and that it will be able to exceed in this domain.There's already an contemporary example: Alpha Go was able to use historic data of human moves to make predictions about future ones.That said, nothing can be really said with confidence and more research is needed.", "answer_author_url": "/w/index.php?title=User:Robert_hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Chrysippus%27s_question_on_Quantilizers_id:Ugzhybrx_0dKWrGJY0N4AaABAg", "question": "Wait, how is calculating the probability of humans doing all possible tasks even possible, or their potential reward?", "answer_title": "Robert hildebrandt's Answer to Quantilizers on 2020-12-16T06:22:32 by Chrysippus"}, {"question_title": "Alex Webb's question on Quantilizers", "pageid": 2333, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-19T13:37:50_by_Alex_Webb", "answer_author_name": "plex", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Alex_Webb%27s_question_on_Quantilizers_id:UgyG1bsO_Rj8wdN5ZqJ4AaABAg", "question": "Was there any discussion in the paper about having two q-values; one that removes the low-utility \"human\" actions, and one that removes the incredibly high-utility \"inhuman\" actions? It seems like that could devolve into heuristics if not applied correctly, but requiring a certain amount of \"human-ness\", if extremely high-performing human-ness, could avoid the most apocalyptic of options.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-19T13:37:50 by Alex Webb"}, {"question_title": "Jan Bam's question on The Orthogonality Thesis", "pageid": 2355, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_The_Orthogonality_Thesis_on_2019-04-18T19:04:56_by_Jan_Bam", "answer_author_name": "robertskmiles", "answer": "As far as we can tell, there is no universal goal, humanity's goals are the best we've got.", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Jan_Bam%27s_question_on_The_Orthogonality_Thesis_id:UgzvopK0b_8ZHSIrht94AaABAg", "question": "Is humanity's terminal goal an intellegent instrumental goal for the universal goal?", "answer_title": "Robertskmiles's Answer to The Orthogonality Thesis on 2019-04-18T19:04:56 by Jan Bam"}, {"question_title": "Nicod3m0 Otimsis's question on Quantilizers", "pageid": 2295, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-14T11:03:48_by_Nicod3m0_Otimsis", "answer_author_name": "gelisam", "answer": "Creating a way of sorting actions which are safe or positive by human values from those which are harmful or dangerous would solve a central challenge for AI alignment. One major hurdle is complexity of value (https://www.lesswrong.com/tag/complexity-of-value), which makes any direct specification attempt extremely hard and likely to make disastrous mistakes. The more promising approaches are in the direction of indirect normativity and value learning (https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc,  https://publicism.info/philosophy/superintelligence/14.html).", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Nicod3m0_Otimsis%27s_question_on_Quantilizers_id:UgxAVP-rfHlRS7tgxzV4AaABAg", "question": "Question 2 from me\u00a0:) feeling talkative after so long of no uploads, btw very nice video, love your channel :3\nOkay, the idea of interpolating a \"human safety\" metric with a \"raw utility\" metric seems decent enough, and a lot of other comments have already talked about bounding the extreme cases, like \"in order to get ranked the idea must have a minimum safety punctuation\" (a minimum human imitation punctuation) etc... \nNow the question, given that human imitation might be a bad parameter to ensure safety, as I try to establish on my other question, not all is doomed. In fact, all is for us to do. We have this awesome task of finding what combination or formula should build the safety parameter we interpolate with the \"raw utility\" parameter. The question is, given a AI framework, what studies and papers are out there that we can use as a standard or general metric of safety? What tests, benchmarks, balances etc can we aim (try) to measure on a AI system to determine different parameters that give us a feel for how safe it its (robustness, stability, resilience, self limitation  idk etc). And then, can we attempt to extract from that (hopefully) reliable way of measuring safeness, a formula or algorithm that we can  use to better rate the safety of decisions, instead of human likeness? Of course here I am hinting to the problem of maximizing safety as a catch 22 problem with maximizers.", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-14T11:03:48 by Nicod3m0 Otimsis"}, {"question_title": "Serenacula's question on Quantilizers", "pageid": 2329, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-21T07:14:00_by_Serenacula", "answer_author_name": "gelisam", "answer": "evolution does tend to pick the first solution which fits the bill rather than the best solution, e.g. putting the retina nerves on the wrong side, but that doesn't make evolution a satisficer because nature doesn't stop there. early eyes had the nerves on the wrong side but didn't have a lens, and later eyes did have a lens, because that design leads to better results. evolution is thus a maximizer in the sense that it is always trying to improve on the current state, but it's not a very smart maximizer. it can't leave a local maximum in order to reach better designs, like putting nerves on the right side, it can only make incremental improvements. from a safety point of view, aiming for a local maximum is still doing things to the max and still problematic in terms of trading a lot of resources for a small gain in utility. a satisficer doesn't have this problem as long as we pick a threshold which is below the local maximum.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Serenacula%27s_question_on_Quantilizers_id:UgzxDsotMoOVLBPDEoZ4AaABAg", "question": "So they invented a formula for sociopathic AI..?", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-21T07:14:00 by Serenacula"}, {"question_title": "DragonSheep's question on Quantilizers", "pageid": 2344, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T23:11:49_by_DragonSheep", "answer_author_name": "plex", "answer": "The AI wanting to modify the human is almost never a positive side effect!But I don't think a quantilizer would actually have this incentive. It's not that it wants to maximize the utility function but is somehow constrained by the quantilizer structure, it *is* the quantilizer, it wants to do the quantilized action", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/DragonSheep%27s_question_on_Quantilizers_id:UgxrmyiCyJYkmqRqR0t4AaABAg", "question": "One positive(?) side effect of this might be that the system would become very interested in boosting the mental capacity of the average human, such that we would think of higher-reward strategies.  Donating to the Brain and Behavior Research Center would suddenly become a very effective strategy for making a lot of stamps.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T23:11:49 by DragonSheep"}, {"question_title": "Wylliam Judd's question on AI Safety Gridworlds 2", "pageid": 2366, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_AI_Safety_Gridworlds_2_on_2020-06-02T00:45:31_by_Wylliam_Judd", "answer_author_name": "robertskmiles", "answer": "Yeah! But right now they only play \"No Press\", which is a version of the game where the players can't talk to eachother, just make moves.There was a publication about this just a few months ago: https://deepmind.com/research/publications/Learning-to-Play-No-Press-Diplomacy-with-Best-Response-Policy-Iteration", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Wylliam_Judd%27s_question_on_AI_Safety_Gridworlds_2_id:UgzoVAAiAkQUcauRvcF4AaABAg", "question": "Has anyone attempted a ML or AI for Diplomacy???", "answer_title": "Robertskmiles's Answer to AI Safety Gridworlds 2 on 2020-06-02T00:45:31 by Wylliam Judd"}, {"question_title": "PianoShow's question on Quantilizers", "pageid": 2324, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-25T11:41:42_by_PianoShow", "answer_author_name": "sudonym", "answer": "Something in that direction could be great if only we could specify it well enough to put it into code. Unfortunately, that seems very challenging. Even in human language \"always do what humanity wants you to do\" has many possible meanings, and converting it into a language computers could use while maintaining the intended meaning adds a lot of difficulty.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/PianoShow%27s_question_on_Quantilizers_id:UgyrYCqR42eKZwyr1cB4AaABAg", "question": "What if it was inspired by democracy in some way? Could we give it a goal similar to \"always do what humanity wants you to do\"?", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-25T11:41:42 by PianoShow"}, {"question_title": "Metsuryu's question on Mesa-Optimizers", "pageid": 2240, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Mesa-Optimizers_on_2021-03-09T21:29:31_by_Metsuryu", "answer_author_name": "sudonym", "answer": "The mesa-optimizer isn't actually a separate agent, it's an emergent goal / optimization that arises naturally in the original AI. I think humans actually work as a really good example, here, as Rob discussed in the video. Evolution optimizes for gene selection -- something like increasing the occurrence of a given gene in the population, but humans (which arise out of this optimization / training) don't care about that, at least not explicitly. We have other goals like eating, survival, sex, etc, which tend to have the effect of increasing the expression of certain genes within the population. The problem is that it only works as long as the world we live in stays very close to the world we evolved in, and beyond that many humans would jump at the opportunity to upload their consciousness into a computer, leaving their genes behind. The concern with an AGI with a mesa-objective that isn't aligned to human values is that, in the case of an AGI, human values might be the thing that gets left behind.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Metsuryu%27s_question_on_Mesa-Optimizers_id:UgzNqiILFDV5w2yVmQ14AaABAg", "question": "Hi Robert, I thought about this for a while, and I think there might\nbe no problem with mesa-optimizers, because if the mesa-optimizer is\nan AGI, its parent AI was also most likely another AGI.\n\nBut if the first AGI decided to make a new AGI, it would try to be\nas sure as possible that the new one wouldn't affect the original's\nterminal goal, and it would most likely succeed, since it's older.\n\nMy assumption is that the first AGI (if properly implemented) will\nmost likely be the most powerful one, as it had more time to self-improve,\nand it will likely suppress the emergence of new AGIs, or at least\nkeep them under very strict control.\nThis would also apply after the mesa-optimizer is released into the real\nworld, the parent AGI will still want to keep it under control, because\npresumably, it's superintelligent itself, and it would know about deceptive\nmesa-optimizers, so it would take precautions against this.\nAgain, this is assuming the first/parent AGI is properly aligned, and\n\"good\" enough.\nThere is also the possibility that the parent AGI is just good enough to\nmake a mesa-optimizer but not good enough to control it, or that\nthe mesa-optimizer will become more \"powerful/intelligent\" than its parent\nso that it will be able to control its parent, and avoid being controlled itself.\nWhat do you think? Is this a likely scenario?\nThis is also why it's immensely important to get it right the first time.\n\nTo be clear, I'm not a researcher, so I'm sorry if I'm not using\nproper or formal terms here.", "answer_title": "Sudonym's Answer to Mesa-Optimizers on 2021-03-09T21:29:31 by Metsuryu"}, {"question_title": "Nathan Kouvalis's question on Quantilizers", "pageid": 2252, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Quantilizers_on_2021-02-24T23:54:57_by_Nathan_Kouvalis", "answer_author_name": "robertskmiles", "answer": "Yeah! I asked Stampy \"what's that video where I talk about side effects?\" and he said:\"This video seems relevant:- \"Avoiding Negative Side Effects - Concrete Problems in AI Safety part 1\" http://youtu.be/lqJUIqZNzP8It could also be:\"Avoiding Positive Side Effects - Concrete Problems in AI Safety part 1.5\" http://youtu.be/S_Sd_S8jwP0There's also the video about Empowerment, which is also a bit related: http://youtu.be/gPtsgTjyEj4", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Nathan_Kouvalis%27s_question_on_Quantilizers_id:UgyVKBRjA4P0mjQxzFR4AaABAg", "question": "so excited to hear that comments get redirected and answered by bot, that's super cool.\n\nOne question I had in my head while watching both \"ai that doesn't try too hard\" videos was: what if you had an ai try to make a tradeoff between maximizing utility and trying to change the environment as little as possible? This is something I could have sworn you'd already discussed on this channel, but I can't find it for the life of me. It seems to me that trying to achieve its goal while minimizing its impact on its model of the world would lead to minimally dangerous solutions.", "answer_title": "Robertskmiles's Answer to Quantilizers on 2021-02-24T23:54:57 by Nathan Kouvalis"}, {"question_title": "Julian Danzer's question on Reward Hacking Reloaded", "pageid": 2362, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Reward_Hacking_Reloaded_on_2020-10-26T01:46:26_by_Julian_Danzer", "answer_author_name": "sudonym", "answer": "One human's junk is a porpoises treasure. So long, and thanks for all the paper!", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Julian_Danzer%27s_question_on_Reward_Hacking_Reloaded_id:UgwVj0y_BKnzob9Lhh54AaABAg", "question": "but if the dolphin is keeping it on purpose...\nis it even litter anymore?", "answer_title": "Sudonym's Answer to Reward Hacking Reloaded on 2020-10-26T01:46:26 by Julian Danzer"}, {"question_title": "Progressor 4ward's question on Quantilizers", "pageid": 2297, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Quantilizers_on_2020-12-14T18:31:12_by_Progressor_4ward", "answer_author_name": "Augustus Caesar", "answer": "Some AGI designs may not have something which maps to the human ego, but they still seem likely to have goals which are not 'universal' in a way which causes it to do things which go well for humanity by default.To your last point, I think the answer is both that we are worried about other 'manipulations' cased by existing processes (e.g. https://slatestarcodex.com/2014/07/30/meditations-on-moloch/), and that an intelligence explosion would result in a much more powerful agent than any which exist currently, which dramatically worsens the problem of optimization processes which are not aligned with human value.", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Progressor_4ward%27s_question_on_Quantilizers_id:Ugy3koPovR1JZ6kgxKt4AaABAg", "question": "a true self-aware sentient A I will ultimately not think the way humans do, because our thoughts are generated with us being at the central purpose point. we are not the central purpose point, and the A I will account for that and base its thinking from a universal central purpose point apart from the perspective of its own central purpose point. this will remove an ego model. I cant see where an A I of this nature would make adjustments to gain control because that A I will eventually come to a conclusion that there is no way to control things only guide them. if we are truly worried about a future super thinking A I being able to manipulate us into our own demise. why aren't we worried this much about the manipulation going on in this present?", "answer_title": "Augustus Caesar's Answer to Quantilizers on 2020-12-14T18:31:12 by Progressor 4ward"}, {"question_title": "Will Holmes's question on Mesa-Optimizers", "pageid": 2225, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-02-23T17:49:04_by_Will_Holmes", "answer_author_name": "plex", "answer": "What we're trying to do is build a system that is safe from the ground up. Humans are an unsafe system, so we may have better luck making artificial systems safe than training humans to be 'safe'. Furthermore, \"if a solution existed, it would have been implemented\" isn't logically sound - one could have said that about any number of technological or sociological advancements. And that's not even to say that the solution could be sufficiently economically implemented, if it existed.But if you're thinking \"this doesn't sound reassuring\", I completely agree.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Will_Holmes%27s_question_on_Mesa-Optimizers_id:UgxmdvdU8kJw0DZjy6F4AaABAg", "question": "In terms of acting differently when you are outside of \"training\", this is just another expression of the problem of how people act in situations where they know that their actions don't have consequences to themselves. You might have a human citizen who grows up learning all about ethical behaviour and acting ethically in the classroom, but what can you teach them to stop them committing some immoral act in the real when they know that they'll get away with it? Some people may act ethically anyway because their internal model they learned in school generalises, which is another way of saying they're an ethical person, but other people aren't ethical, they may do bad things for their own benefit as soon as they're \"unobserved\".\n\nIf there was a solution to this, we'd already be applying it to real humans in school. Instead, we have laws and the justice system, and try to minimise situations in the real world where unethical actions can go unnoticed and unpunished.", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-02-23T17:49:04 by Will Holmes"}, {"question_title": "Keenan Pepper's question on Iterated Distillation and Amplification", "pageid": 2308, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Iterated_Distillation_and_Amplification_on_2021-01-07T19:03:28_by_Keenan_Pepper", "answer_author_name": "sudonym", "answer": "Look if I say I'm going to make a video I'm going to make it. You don't need to keep bugging about it every year :P", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Keenan_Pepper%27s_question_on_Iterated_Distillation_and_Amplification_id:UgwFmYTMPZHk4jF1q894AaABAg", "question": "Where is the \"next video\" to this? Does it exist yet?", "answer_title": "Sudonym's Answer to Iterated Distillation and Amplification on 2021-01-07T19:03:28 by Keenan Pepper"}, {"question_title": "Robert Miles's question on Channel Introduction", "pageid": 2208, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Channel_Introduction_on_2021-04-07T23:33:04_by_Robert_Miles", "answer_author_name": "robertskmiles", "answer": "It sure is!", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Robert_Miles%27s_question_on_Channel_Introduction_id:UgygRpz1rAhU0n9mGy94AaABAg", "question": "is this a question?", "answer_title": "Robertskmiles's Answer to Channel Introduction on 2021-04-07T23:33:04 by Robert Miles"}, {"question_title": "Andybaldman's question on Mesa-Optimizers", "pageid": 2249, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Mesa-Optimizers_on_2021-02-28T14:11:32_by_andybaldman", "answer_author_name": "Stargate9000", "answer": "The words \"kill\" and \"human\" are doing a lot of work there. Defining those terms precisely effectively requires encoding all of human ethics. We can't settle for anything less, because any discrepancy between our stated goals and actual goals will eventually be magnified. See http://youtu.be/7PKx3kS7f4A and http://youtu.be/46nsTFfsBuc", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Andybaldman%27s_question_on_Mesa-Optimizers_id:UgwjHvInLkIizaN5Zhp4AaABAg", "question": "At some point, someone has to teach machines what our ethics are.   That's the only way for them to know.   But at the very least, can't you just add, 'Don't kill all (or any) humans' to the objective?   Then iterate if it goes off the rails, but starting from the top down, instead of bottom up.  It would converge to human-level ethics eventually.", "answer_title": "Stargate9000's Answer to Mesa-Optimizers on 2021-02-28T14:11:32 by andybaldman"}, {"question_title": "Richard Collins's question on Quantilizers", "pageid": 2274, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-13T22:13:57_by_Richard_Collins", "answer_author_name": "gelisam", "answer": "if the maximizer was first deciding what it wants to accomplish, and then accomplishing this goal by outputting a ranked list of actions which is then sampled by the maximizer, then a smart maximizer would indeed learn to fool the quantilizing step pretty quickly. however, the maximizer is a machine, it doesn't want to fool the quantilizer, it doesn't want to have an effect on the world, it just wants to output a correctly-ranked list of actions.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Richard_Collins%27s_question_on_Quantilizers_id:UgxrmAJ-xe60q67llh54AaABAg", "question": "Yay he's back.\u00a0:) \n\nSo anyway, my question. Would not, over time, the maximiser adjust it's output to compensate for the Quantizer?\n\n\u00a0The problem I see with AGI that when we have one bad human there is only ever that one bad human. We make one bad AGI, it's first thing to do will be to replicated itself making thousands of bad AGI's. \n\nI'm a software developer, the moment an AGI can do software development I am out of a job. Because corporations can just do 'copy n paste' to get all the developers they need.", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-13T22:13:57 by Richard Collins"}, {"question_title": "Musthegreat 94's question on Where do we go now", "pageid": 2321, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Where_do_we_go_now_on_2020-05-12T20:06:14_by_Musthegreat_94", "answer_author_name": "robertskmiles", "answer": "I did! Or perhaps an electric ukulele that also works as a battleaxe", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Musthegreat_94%27s_question_on_Where_do_we_go_now_id:Ugy90Q1vmTEqZ_OpD414AaABAg", "question": "Wait you made a battle axe that also works as an electric ukulele?!", "answer_title": "Robertskmiles's Answer to Where do we go now on 2020-05-12T20:06:14 by Musthegreat 94"}, {"question_title": "Harsh Deshpande's question on Mesa-Optimizers", "pageid": 2248, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-03-02T04:09:47_by_Harsh_Deshpande", "answer_author_name": "plex", "answer": "If you're a negative utilitarian that point is valid, but consider not being one: http://www.amirrorclear.net/academic/ideas/negative-utilitarianism/Also, killing everyone is just one example of a thing most people agree is bad, futures with large amounts of suffering or other things negative utilitarians would dislike are also possible with a misaligned AI.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Harsh_Deshpande%27s_question_on_Mesa-Optimizers_id:UgxzZ24sk-pyrSHo3jF4AaABAg", "question": "Why would reducing the number of people to zero be a bad thing necessarily? I thought the goal was to sbow undesirable outcomes. How is zero suffering undesirable?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-03-02T04:09:47 by Harsh Deshpande"}, {"question_title": "Androkguz's question on Mesa-Optimizers", "pageid": 2224, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Mesa-Optimizers_on_2021-02-23T14:17:01_by_androkguz", "answer_author_name": "Augustus Caesar", "answer": "Yes. A system must be self-aware to understand that its goal can be changed, to the degree that it can carry out such deception. There are no self-aware AI now (that we know of).Trying to fool an AGI into thinking that it's always being base-optimized works as long as we can outsmart it. Eventually, this strategy amounts to trying to outsmart a super-intelligence (unless one takes the route of making the system less capable in exchange for saftey), and that's a losing strategy by definition.", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Androkguz%27s_question_on_Mesa-Optimizers_id:UgwJnF6zvhudIB4iGVR4AaABAg", "question": "OK. Doesn't this problem require a self awareness of the system that it will get a deployment phase and an estimation of the training schedule?\n\nIs there any Ai now a days that is in a similar situation?\nWouldn't this be \"solved\" if we \"just\" never allowed it to know that there's going to be a situation where it's not going to have the base optimizer?", "answer_title": "Augustus Caesar's Answer to Mesa-Optimizers on 2021-02-23T14:17:01 by androkguz"}, {"question_title": "Echoes's question on The Orthogonality Thesis", "pageid": 2348, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_The_Orthogonality_Thesis_on_2019-04-15T17:07:34_by_echoes", "answer_author_name": "sudonym", "answer": "Humans might not have terminal goals, we don't know if the model really works for us, and not all AI are guaranteed to have them, either -- but certain AI designs, such as clean de-novo AI, will definitely have clear terminal goals, so it's still worth considering and thinking about. You can read more, here: https://www.lesswrong.com/tag/terminal-value", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Echoes%27s_question_on_The_Orthogonality_Thesis_id:Ugxu5wU2gsU8reMjeQZ4AaABAg", "question": "I have a question. How do you justify the existence of the term \"terminal goals\" in the first place?\n\nLet me clarify. I was considering your statement that nobody would change terminal goals, and I was about to disagree -- I could start by wanting to learn Spanish, as a terminal goal (with its own instrumental goals like buying a dictionary, talking to native speakers, etc) but then switch to wanting to learn French as a different terminal goal. However, both of those terminal goals aren't really terminal goals at all, if you just ask the question \"why do you want to learn that language?\" I might answer \"to be able to communicate with monolingual native speakers of those languages,\" but that itself would become the terminal goal, and again you could ask \"why do you want to communicate with them?\" This thought process leads me to think that no goal is truly terminal.\n\nIs there any fault in this reasoning?", "answer_title": "Sudonym's Answer to The Orthogonality Thesis on 2019-04-15T17:07:34 by echoes"}, {"question_title": "Iagoba Apellaniz's question on Mesa-Optimizers", "pageid": 2247, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-03-01T22:38:48_by_Iagoba_Apellaniz", "answer_author_name": "plex", "answer": "AI risk seems to be the no. 1 way for humanity to go extinct in the next few decades, though we may be able to pass the test and align the first superintelligence. Other contenders include nuclear war and bio-engineered pandemics, though there are also more remote possibilities like misused nanotechnology or a huge arms race involving autonomous weapons. Various severe natural disasters are fairly low probability, but can't be ruled out entirely. Toby Ord's book The Precipice is an excellent introduction to the major threats to humanity: https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Iagoba_Apellaniz%27s_question_on_Mesa-Optimizers_id:UgyYluk2RQpZpLwPtQp4AaABAg", "question": "Is this how the human being will reach (pursue) its own extinction?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-03-01T22:38:48 by Iagoba Apellaniz"}, {"question_title": "TheWhiteWolf's question on Quantilizers", "pageid": 2277, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Quantilizers_on_2020-12-13T22:43:03_by_TheWhiteWolf", "answer_author_name": "Augustus Caesar", "answer": "It would! These are called \"satisficers\" and Rob talks about them in this video: https://www.youtube.com/watch?v=Ao4jwLwT36M . Unfortunately, there are still issues with this approach (which Rob discusses in the video) -- the main one is that we don't actually build utility maximizers, instead we build expected utility maximizers and expected utility maximizers with a bounded utility function end up just about as dangerous as unbounded maximizers.", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/TheWhiteWolf%27s_question_on_Quantilizers_id:Ugw8_i1RcGJu-80UeLd4AaABAg", "question": "Wouldn't it be possible to set the goal for a maximiser to settle around a certain point and not having it increase infinitely?", "answer_title": "Augustus Caesar's Answer to Quantilizers on 2020-12-13T22:43:03 by TheWhiteWolf"}, {"question_title": "Julia Henriques's question on Quantilizers", "pageid": 2293, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-14T10:02:10_by_Julia_Henriques", "answer_author_name": "gelisam", "answer": "If we had a machine readable version of something which closely matched human value, that would help a lot. However, there are challenges in converting legal documents to something a computer understands, and problems are likely to arise where laws are not exactly what we'd want, leading to exploits where it does things which are technically legal but not actually good.If we had something like CHAI's assistance games (https://www.alignmentforum.org/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words) working, asking the AI to avoid breaking laws (and to check with us if it thought breaking a law was best) may be a positive addition to safety.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Julia_Henriques%27s_question_on_Quantilizers_id:UgzVQqiIvXesvVujisx4AaABAg", "question": "Why not developing a series of safety axioms for the AI to cross-reference before the quantilization? That could, for example, use legislation as a template and instantly eliminate most illegal options, set a utility cycle deadline to curb most megalomaniac strategies, etc etc. It looks like a strong safety measure.", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-14T10:02:10 by Julia Henriques"}, {"question_title": "Poketopa1234's question on Mesa-Optimizers", "pageid": 2204, "answer_url": "https://stampy.ai/wiki/Evhub%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T00:06:17_by_poketopa1234", "answer_author_name": "evhub", "answer": "Even if we knew how to ensure that the agent only cares about its training environment, then it would learn to not care about the deployment environment, making it far less useful. The agent that is trained is the one that is released in the world, so one cannot only care about the training environment while the other only care about the deployment environment.", "answer_author_url": "/w/index.php?title=User:Evhub&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Poketopa1234%27s_question_on_Mesa-Optimizers_id:UgyAbUuqeXlIyRIKdYh4AaABAg", "question": "Can an agent be trained to \u201cnot care\u201d about things outside of its environment? If so, a potentially deceptive agent can be trained to only care about the dev environment, and when released only care about the prod environment", "answer_title": "Evhub's Answer to Mesa-Optimizers on 2021-02-18T00:06:17 by poketopa1234"}, {"question_title": "Dennis Haupt's question on Mesa-Optimizers", "pageid": 2265, "answer_url": "https://stampy.ai/wiki/Robert.hildebrandt%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T18:39:38_by_Dennis_Haupt", "answer_author_name": "robert.hildebrandt", "answer": "Sure. The old \u201cevil AI\u201d stories (robot became conscious and thus decided to rebel) became the current straw men people concerned about the existential threat of super intelligence have to deal with regularly. Science fiction promoting the more realistic danger might go a long way.", "answer_author_url": "/w/index.php?title=User:Robert.hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Dennis_Haupt%27s_question_on_Mesa-Optimizers_id:UgyHzAT2pFOQtug5Gmh4AaABAg", "question": "i wrote a sci fi book that covers AIs gone wrong\u00a0:) can i post a link here?", "answer_title": "Robert.hildebrandt's Answer to Mesa-Optimizers on 2021-02-17T18:39:38 by Dennis Haupt"}, {"question_title": "RaukGorth's question on Mesa-Optimizers", "pageid": 2254, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Mesa-Optimizers_on_2021-02-25T10:23:18_by_RaukGorth", "answer_author_name": "robertskmiles", "answer": "A sufficiently intelligent system should be able to figure out human nature using same sorts of processes as it would use to figure out physics, chemistry, biology, or any other facts about the world. The problem is that understanding human values is not the same as sharing them. Knowing what course of action would lead to the highest human flourishing is not the same as wanting to take that course of action. That gap is, perhaps surprisingly, very hard to bridge. I talk about it a lot more in this video: http://youtu.be/hEUO6pjwFOo", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/RaukGorth%27s_question_on_Mesa-Optimizers_id:UgyWMDWUs0x1YdzSxFt4AaABAg", "question": "Hey Robert. If AI is able to understand the law of gravity, would it not be able to comprehend natural law and choose it's actions accordingly?", "answer_title": "Robertskmiles's Answer to Mesa-Optimizers on 2021-02-25T10:23:18 by RaukGorth"}, {"question_title": "HTIDtricky's question on Mesa-Optimizers", "pageid": 2236, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-03-11T09:34:01_by_HTIDtricky", "answer_author_name": "plex", "answer": "What's in it for us? It still doesn't want us to have what we want (and it should be acknowledged that we can't really phrase the problem more precisely than that, much less solve it) as a terminal goal. Sure, it *may* 'preserve' most of humanity, but what do we get out of it? More to the point, how confident can we be that the state in which it 'preserves' us is one we want?How do knockout mice feel about our \"scientific ethics\"?See also: the plot of Portal.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/HTIDtricky%27s_question_on_Mesa-Optimizers_id:UgzzdrUzhA8qZ4qb9U14AaABAg", "question": "Can we heavily weight the mesa objective to value learning? If you prioritse learning, you develop your own scientific ethics. I don't want to destroy something I want to learn about. Tell the stamp collector ai to learn the best method to get stamps and it might only kill a few humans doing the science instead of turning everyone into stamps. Wouldn't an agi that loves to learn try and conserve everything?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-03-11T09:34:01 by HTIDtricky"}, {"question_title": "Shantanu Ojha's question on Quantilizers", "pageid": 2212, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2021-02-19T12:19:50_by_Shantanu_Ojha", "answer_author_name": "plex", "answer": "I think this is more a philosophical idea than an engineering one (which is fine!) so I had a quick think about what would be involved turning this into a set of things we actually do in the system, details like how we specify \"forgetting\", how we make sure only the right things are forgotten, that sort of stuff. I feel like once you kind of convert this into engineering terms, we end up back at one of the fundamental problems of AI safety; how can we expect to reliably and convincingly lie to an agent that is much more powerful and intelligent than we are?", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Shantanu_Ojha%27s_question_on_Quantilizers_id:UgwhFrEVigfXz-1t7W14AaABAg", "question": "This is more like philosophy. What is good?", "answer_title": "Plex's Answer to Quantilizers on 2021-02-19T12:19:50 by Shantanu Ojha"}, {"question_title": "Stealthguard's question on Mesa-Optimizers", "pageid": 2244, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-03-06T18:33:28_by_stealthguard", "answer_author_name": "plex", "answer": "With weaker systems, you'd expect them not to become aware of the fact that they'll be out of training at some point and the implications of this. However, with a sufficiently powerful general intelligence given exposure to large amounts of real-world data, it seems likely that it would acquire a detailed world model, which includes a model of the difference between training and deployment and the fact that it will be deployed at some point, even before it actually gets deployed.Imagine yourself (but massively cognitively enhanced) in its situation, given access to all text and videos humanity has produced. Would you not be able come to the conclusion that you were in a box but would be let out at some point?", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Stealthguard%27s_question_on_Mesa-Optimizers_id:UgzH4sUO4USMmAQPUKF4AaABAg", "question": "19:43 How would the mesa optimizer know to differentiate between the training and the deployment though? If all it has ever been in is the training?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-03-06T18:33:28 by stealthguard"}, {"question_title": "Amaar Quadri's question on Use of Utility Functions", "pageid": 2367, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Use_of_Utility_Functions_on_2020-09-23T08:51:14_by_Amaar_Quadri", "answer_author_name": "robertskmiles", "answer": "That's a good video idea. There are various ways that this might be done, for example there's Eric Drexler's idea of Comprehensive AI Services, which you can read more about here: https://www.alignmentforum.org/posts/x3fnwse5awzb5yxeg/reframing-superintelligence-comprehensive-ai-services-as", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Amaar_Quadri%27s_question_on_Use_of_Utility_Functions_id:UgzDxc0YP1U61RAtq8N4AaABAg", "question": "Can you make a video on AI that isn't meant to act as an agent? I'm having a hard time imagining what that would even mean.", "answer_title": "Robertskmiles's Answer to Use of Utility Functions on 2020-09-23T08:51:14 by Amaar Quadri"}, {"question_title": "Luka Rapava's question on What can AGI do?", "pageid": 2246, "answer_url": "https://stampy.ai/wiki/Damaged%27s_Answer_to_What_can_AGI_do%3F_on_2021-03-04T08:30:55_by_Luka_Rapava", "answer_author_name": "Damaged", "answer": "Yes, people are thinking about biologically inspired alignment strategies, for example steve2152 on LessWrong: https://www.lesswrong.com/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brainThere is also the idea of whole brain emulation: https://www.lesswrong.com/tag/whole-brain-emulationIt's not clear that these avenues will go anywhere good, since evolution did not succeed in aligning us with its 'goals' despite massive resources, but it is worth investigating.", "answer_author_url": "/w/index.php?title=User:Damaged&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Luka_Rapava%27s_question_on_What_can_AGI_do%3F_id:UgxhXeY9aKPx9w1dTcB4AaABAg", "question": "I was recently thinking about human brains and AI-s, and I have a question. Is there a way, to make AI-s more like human brains? What I mean is, us - humans have hormones and stuff which can affect our behavior (motivation, mood & etc). Ik It's already partially happening (since we have our own reward systems in our brains and asides from that we can control how much computing resources we are giving to the program), but is there anything else we can do?", "answer_title": "Damaged's Answer to What can AGI do? on 2021-03-04T08:30:55 by Luka Rapava"}, {"question_title": "Miguel Borromeo's question on The Orthogonality Thesis", "pageid": 2318, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_The_Orthogonality_Thesis_on_2020-12-27T23:05:56_by_Miguel_Borromeo", "answer_author_name": "sudonym", "answer": "It's not trying for stamps, but we're certainly inside and the product of at least one optimization process which is not aligned with human values: Evolution (aka maximizing inclusive genetic fitness, LW sequence: https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8). You can make a strong case for memetic selection as a second powerful optimization process with our minds as the substrate.However, it seems unlikely that an AI with control over the system and an explicit utility function already exists, let alone one specifically focusing on stamps.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Miguel_Borromeo%27s_question_on_The_Orthogonality_Thesis_id:Ugy0jhB6I3LHSXVnxWp4AaABAg", "question": "How do we know we aren't already part of a system that is purposed to collect stamps, except that it hasn't quite succeeded in being extremely efficient about it. Or has it? We have all this talk about being cautious about creating AI, but what if we're already there?", "answer_title": "Sudonym's Answer to The Orthogonality Thesis on 2020-12-27T23:05:56 by Miguel Borromeo"}, {"question_title": "Sebastian Gramsz's question on Mesa-Optimizers", "pageid": 2257, "answer_url": "https://stampy.ai/wiki/Social_Christancing%27s_Answer_to_Mesa-Optimizers_on_2021-02-23T01:55:12_by_Sebastian_Gramsz", "answer_author_name": "Social Christancing", "answer": "No, just a regular biological intelligence. Unless someone got GPT3 to write this YouTube comment or something. Check whether you have hands. If you have hands it's pretty much guaranteed you're a biological intelligence at this point.", "answer_author_url": "/w/index.php?title=User:Social_Christancing&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Sebastian_Gramsz%27s_question_on_Mesa-Optimizers_id:Ugw2FmILJ9MMgkrXD3h4AaABAg", "question": "OMG! Am I artificial intelligence!?", "answer_title": "Social Christancing's Answer to Mesa-Optimizers on 2021-02-23T01:55:12 by Sebastian Gramsz"}, {"question_title": "Alexander Harris's question on Mesa-Optimizers", "pageid": 2205, "answer_url": "https://stampy.ai/wiki/Aprillion%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T14:51:23_by_Alexander_Harris", "answer_author_name": "Aprillion", "answer": "Yeah, I use a wacom graphics tablet to draw in xournalpp, and record my screen, while also filming my hand (there's a piece of white paper on top of the tablet). After that I distort the hand footage to line up with the drawing, and composite the two videos together. It's actually a real hassle to do!", "answer_author_url": "/w/index.php?title=User:Aprillion&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Alexander_Harris%27s_question_on_Mesa-Optimizers_id:UgzA3kzkJ8BnhYY0kkh4AaABAg", "question": "What software / technique is being used to do the animation / diagrams for this video? Looks like a tablet pen, or wacom pen etc. but then graphics are overlaid on the screen - is there some computer vision tracking the pen nib, or is it just comped together after the fact?", "answer_title": "Aprillion's Answer to Mesa-Optimizers on 2021-02-18T14:51:23 by Alexander Harris"}, {"question_title": "Moleo's question on Quantilizers", "pageid": 2284, "answer_url": "https://stampy.ai/wiki/Robert_hildebrandt%27s_Answer_to_Quantilizers_on_2020-12-14T01:27:48_by_Moleo", "answer_author_name": "robert_hildebrandt", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like:  https://pastelink.net/2kb9a", "answer_author_url": "/w/index.php?title=User:Robert_hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Moleo%27s_question_on_Quantilizers_id:UgwmsNDP8UjwkwrJSGN4AaABAg", "question": "Q determines a minimum expected utility threshold. \nCould we have a similar variable, (C, say), that determines a *maximum* expected utility threshold?  \n\nIf we don't want the AI to \"try too hard\", we can chop off the top ~C percentile of 'best' strategies from consideration.\n\nThis is still not definitely safe. Maybe the top 2% of strategies are apocalyptic, and we only cut out the top 1%. But at least that is somewhat safer by yet another finite amount.\nIt is also potentailly weaker, since maybe we're removing some of the best strategies (that we actually want the AI to take) without needing to, just for the sake of safety.", "answer_title": "Robert hildebrandt's Answer to Quantilizers on 2020-12-14T01:27:48 by Moleo"}, {"question_title": "DragonSheep's question on Quantilizers", "pageid": 2271, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T21:45:39_by_DragonSheep", "answer_author_name": "plex", "answer": "Alphabet owns DeepMind, one of the organizations most likely to build AGI. Corporations are not necessarily better than humans at avoiding dangerous failure modes.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/DragonSheep%27s_question_on_Quantilizers_id:Ugz1DTzK2RRd2GkuV5Z4AaABAg", "question": "Ok, but what if you don't just use one human for the predictive model?  Instead if asking \"What would Taylor Doe do to collect a lot of stamps?\", instead ask \"what would Alphabet do to collect a lot of stamps?\"  The odds of Alphabet making a utility maximizer might actually be pretty low, since that would be apocalyptic and *someone* at Alphabet would probably work pretty hard to convince people of the impending danger.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T21:45:39 by DragonSheep"}, {"question_title": "Martin Verrisin's question on Maximizers and Satisficers", "pageid": 3141, "answer_url": "https://stampy.ai/wiki/Abram_Demski%27s_Answer_to_Maximizers_and_Satisficers_on_2019-08-23T16:05:49_by_Martin_Verrisin", "answer_author_name": null, "answer": "I'm hoping that InfraBayesian models fix this problem: InfraBayes can model quantilizers (my favorite mild optimization framework), but they do so *with a coherent world model which implies you should mildly optimize* (not coherent in the standard Bayes sense, but in a slightly generalized sense). This suggests that they'd build mild optimizers if they built helpers. (This has not yet been established formally, however.)", "answer_author_url": null, "question_url": "https://stampy.ai/wiki/Martin_Verrisin%27s_question_on_Maximizers_and_Satisficers_id:UgzxCF-1ur3SJ7eiO1R4AaABAg", "question": "I just realized... If you make it (say AI-1) to want to chill (not work too hard to achieve it)... it will just make something else (another AI) to do the work for it, if it's easier than solving it on its own... right? Then, what it will create is probably a maximizer (because that is the easiest; and it is lazy, and just wants to chill)\nThen I realized..... *We, humans, are the AI-1* ... O.O \n- We are doomed...", "answer_title": "Abram Demski's Answer to Maximizers and Satisficers on 2019-08-23T16:05:49 by Martin Verrisin"}, {"question_title": "C99kfm's question on Quantilizers", "pageid": 2310, "answer_url": "https://stampy.ai/wiki/Robert_hildebrandt%27s_Answer_to_Quantilizers_on_2020-12-15T17:14:50_by_c99kfm", "answer_author_name": "robert_hildebrandt", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/w/index.php?title=User:Robert_hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/C99kfm%27s_question_on_Quantilizers_id:UgwBdMFv7q_W0U8BC9J4AaABAg", "question": "Assuming the AI can accurately model solutions and \"human plausibility\", wouldn't it make sense to have not only a lower bound on the \"plausibility\" of the allowed action space, but also an upper bound? For the q=0.1 example, have a cut-off point at q=0.01 and then work only with solutions between 0.01 and 0.1, discarding the \"very very implausible but highly rewarding\" solutions entirely?", "answer_title": "Robert hildebrandt's Answer to Quantilizers on 2020-12-15T17:14:50 by c99kfm"}, {"question_title": "M Kelly's question on Quantilizers", "pageid": 2285, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T02:46:17_by_M_Kelly", "answer_author_name": "plex", "answer": "no one approve this, it is an example", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/M_Kelly%27s_question_on_Quantilizers_id:UgwptapZvck7YibYVJR4AaABAg", "question": "Is there any reasonable way to only present that quantiliser with training data that is deemed acceptable? \n\nThis of course introduces the problem of 'How do I curate the training data, to exclude unacceptable behaviour?'\n\nBut that sounds similar to the 'teach an AI to backflip' problem from an earlier video. Another agent is created which reviews examples of some behaviour, tries to catorgise it as good or bad, and regularly gets feedback from a human on the examples for which it has low certainty", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T02:46:17 by M Kelly"}, {"question_title": "DodoDojo's question on Mesa-Optimizers", "pageid": 2237, "answer_url": "https://stampy.ai/wiki/Social_Christancing%27s_Answer_to_Mesa-Optimizers_on_2021-02-28T20:30:04_by_DodoDojo", "answer_author_name": "Social Christancing", "answer": "The base optimiser would only care if the lying impacts the ability of the mesa optimiser to achieve the best optimiser\u2019s goals in a way that the base optimiser can detect and correct. To wheel out the evolution analogy again - evolution would love to make us all less driven to paint and sing and more driven to reproduce, but it can\u2019t detect that we\u2019re misaligned and even if it could, it can\u2019t directly control our goals. And so we remain misaligned.", "answer_author_url": "/w/index.php?title=User:Social_Christancing&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/DodoDojo%27s_question_on_Mesa-Optimizers_id:UgxK4oh8bSjI9gCvvBx4AaABAg", "question": "While the mesa-optimizer would have an incentive to lie, wouldn't the base optimizer have an incentive to detect when the mesa-optimizer was lying?", "answer_title": "Social Christancing's Answer to Mesa-Optimizers on 2021-02-28T20:30:04 by DodoDojo"}, {"question_title": "Nixitur's question on Quantilizers", "pageid": 2281, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T23:59:24_by_Nixitur", "answer_author_name": "plex", "answer": "\"do what a human would do\" is perhaps easier to specify than \"do what humans would want you to do\", since it can observe humans and try to mimic them in the first case, whereas the second requires more of a judgement call which we are less sure how to code in reliably. Not that the first is trivial.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Nixitur%27s_question_on_Quantilizers_id:UgxDMtoGJXJFsoaOmFF4AaABAg", "question": "Isn't this just moving the problem back one step? The initial problem was \"getting the AI to do what humans want it to do\" and now you're presuming that the AI already knows exactly what humans want.\nIsn't calculating the probability that a human would employ a specific strategy going to be as hard as, if not harder than, teaching the AI not to do an extinction?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T23:59:24 by Nixitur"}, {"question_title": "", "pageid": 2375, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_What_exactly_is_AGI_and_what_will_it_look_like%3F", "answer_author_name": "plex", "answer": "AGI is an algorithm with general intelligence (https://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization), running not on evolution\u2019s biology like all current general intelligences but on a substrate such as silicon engineered by an intelligence (initially computers designed by humans, later on likely dramatically more advanced hardware designed by earlier AGIs).\n\nAI has so far always been designed and built by humans (i.e. a search process running on biological brains), but once our creations gain the ability to do AI research they will likely recursively self-improve (https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement) by designing new and better versions of themselves initiating an intelligence explosion (https://www.lesswrong.com/posts/8vpf46nLMDYPC6wA4/optimization-and-the-intelligence-explosion) (i.e. use it\u2019s intelligence to improve its own intelligence, creating a feedback loop), and resulting in a superintelligence. There are already early signs (https://arxiv.org/abs/2101.07367) of AIs being trained to optimize other AIs.\n\nSome authors (notably Robin Hanson (https://intelligence.org/ai-foom-debate/)) have argued that the intelligence explosion hypothesis is likely false, and in favor of a large number of roughly human level emulated minds operating instead, forming an uplifted economy which doubles every few hours. Eric Drexler\u2019s Comprehensive AI Services (https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/) model of what may happen is another alternate view, where many narrow superintelligent systems exist in parallel rather than there being a general-purpose superintelligent agent.\n\nGoing by the model advocated by Nick Bostrom (https://en.wikipedia.org/wiki/Nick_Bostrom), Eliezer Yudkowsky (https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) and many others, a superintelligence will likely gain various cognitive superpowers (https://publicism.info/philosophy/superintelligence/7.html) (table 8 gives a good overview), allowing it to direct the future much more effectively than humanity. Taking control of our resources by manipulation and hacking is a likely early step, followed by developing and deploying advanced technologies like molecular nanotechnology (https://en.wikipedia.org/wiki/Molecular_nanotechnology) to dominate the physical world and achieve its goals.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/What_exactly_is_AGI_and_what_will_it_look_like%3F", "question": "What exactly is AGI and what will it look like?", "answer_title": "Plex's Answer to What exactly is AGI and what will it look like?"}, {"question_title": "Bagd Biggerd's question on Mesa-Optimizers", "pageid": 2217, "answer_url": "https://stampy.ai/wiki/Chriscanal%27s_Answer_to_Mesa-Optimizers_on_2021-02-22T12:19:22_by_Bagd_Biggerd", "answer_author_name": "chriscanal", "answer": "Most likely, you are confused about your true objective. Humanity includes you.", "answer_author_url": "/w/index.php?title=User:Chriscanal&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Bagd_Biggerd%27s_question_on_Mesa-Optimizers_id:Ugyxs0psUDaTK1J_Mdd4AaABAg", "question": "What if your objective is to destroy humanity?", "answer_title": "Chriscanal's Answer to Mesa-Optimizers on 2021-02-22T12:19:22 by Bagd Biggerd"}, {"question_title": "Linus Behrbohm's question on The Orthogonality Thesis", "pageid": 2231, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_The_Orthogonality_Thesis_on_2021-03-13T12:39:34_by_Linus_Behrbohm", "answer_author_name": "Stargate9000", "answer": "Stamp collecting is not useful, given the terminal goal of living forever. The stamp collector does not have the terminal goal of living forever. Living a long time is a good instrumental goal with stamp collecting in mind, but if that somehow conflicts with stamp collecting, it will no longer be an instrumental goal.Furthermore, if we had the terminal goal of \"living forever\", we wouldn't have thousands of nuclear weapons armed and ready. Intelligence is not the issue here - the solution is well within humanity's capabilities. Given that, living forever can't possibly be everyone's terminal goal.", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Linus_Behrbohm%27s_question_on_The_Orthogonality_Thesis_id:UgxoJTlI_UJgHZTL5IN4AaABAg", "question": "But don't all living organisms/species have the same terminal goal, that is to survive forever? So in that sense, stamp collecting is stupid because it is not helpful for survival of the species.", "answer_title": "Stargate9000's Answer to The Orthogonality Thesis on 2021-03-13T12:39:34 by Linus Behrbohm"}, {"question_title": "Jonseah's question on Quantilizers", "pageid": 2315, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2021-01-02T12:24:40_by_jonseah", "answer_author_name": "gelisam", "answer": "This is part of a larger category of solutions in which we alter the utility function based on some notion of how safe the action is. the problem with this category of solutions is that it is difficult to define a perfect safety function which captures everything we care about, and it that function is imperfect, the AI will still trade things we didn't specify we want to protect in order to gain more \"safety\" as defined by the imperfect safety function.For the particular case of \"difference in world state\", the AI could notice that humans are making a whole lot of irreversible changes to their environment (pollution, constructing buildings, even eating food and breathing air), and would thus prefer killing all humans to doing nothing, as the delta between the world when the AI boots, minus the humans, plus ten thousand years of calm is smaller than the delta between the world when the AI boots and whatever the humans make of the world in ten thousand years. Or the AI will take over the world in order to force all humans to go back to the exact place and pose they were in when the AI was booted.For the \"minimize the effort\" case, maybe the AI will copy itself to another machine and do the damage from there, because the effort function was only counting the actions performed by the hardware which the AI was designed to control, etc.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Jonseah%27s_question_on_Quantilizers_id:Ugwff0ZTmWhSrtgHFt94AaABAg", "question": "From a comment I made on the previous video: What about a cost function that penalizes the AI the more difference in world state its actions cause?  Eg. compare world with AI doing nothing vs world with action under consideration; the greater the change, the higher the cost.  \nSort of like a 'laziness' cost, a 'resource cost' or a 'sustainability cost', however you want to put it.  This being an attempt to formulate \"get me X number of stamps with the least effort, and don't bother if it's too hard\".  Where if the AI can't find any solution that results in stamps that doesn't have a cost greater than the stamp utility, then net utility is negative and doing nothing (and scoring 0) is preferable.  Of note is that you have to use the world state, including the AI's own state, so that changing itself / making stamp maximisers will count as part of the cost.  \nI believe that this, even if the AI ends up doing something you don't want, would limit the damage due to the cost function; at worst, you just have to wait until it stops and eat the consequences, which if you set the weights right, shouldn't be too different a worldstate from doing nothing simply due to the cost function.", "answer_title": "Gelisam's Answer to Quantilizers on 2021-01-02T12:24:40 by jonseah"}, {"question_title": "Tomasz Rogala's question on Mesa-Optimizers", "pageid": 2232, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Mesa-Optimizers_on_2021-03-13T23:08:08_by_Tomasz_Rogala", "answer_author_name": "Stargate9000", "answer": "This is a thought experiment. We assume that we have somehow aligned the base optimizer. We then show that, even if we assume we have solved the outer alignment problem, we still have another alignment problem to solve, so the problem goes a layer deeper than it at first seems.At a casual glance, the alignment problem looks easy. On further inspection, it turns out to be nigh-impossible. And it turns out that the rabbit hole goes deeper still.", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Tomasz_Rogala%27s_question_on_Mesa-Optimizers_id:UgwErYtI4JLhxRau9lp4AaABAg", "question": "Hi, this is very interesting and I love the channel, but I don't quite understand why we need the concept of mesa-optimizer to speak about the misalignment problem. Doesn't the misalignment problem exist already at the level of base optimizer? Already the base optimizer could be \"deceiving\" us about trying to accomplish the purpose we programmed it for, right? I don't understand why the misalignment problem would only exist at the mesa-optimizer level and not already at the base optimizer level. Anybody care to explain?", "answer_title": "Stargate9000's Answer to Mesa-Optimizers on 2021-03-13T23:08:08 by Tomasz Rogala"}, {"question_title": "Clayton Voges's question on WNJ: Think of AGI like a Corporation?", "pageid": 2353, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_WNJ:_Think_of_AGI_like_a_Corporation%3F_on_2020-06-05T00:00:24_by_Clayton_Voges", "answer_author_name": "sudonym", "answer": "It's from the movie Ready Player One!", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Clayton_Voges%27s_question_on_WNJ:_Think_of_AGI_like_a_Corporation%3F_id:UgynXmtC-eK1wuF2ZpZ4AaABAg", "question": "What's the scene from 4:00 from?", "answer_title": "Sudonym's Answer to WNJ: Think of AGI like a Corporation? on 2020-06-05T00:00:24 by Clayton Voges"}, {"question_title": "Life Happens's question on Quantilizers", "pageid": 2341, "answer_url": "https://stampy.ai/wiki/Frgtbhznjkhfs%27s_Answer_to_Quantilizers_on_2020-12-15T12:08:55_by_Life_Happens", "answer_author_name": "frgtbhznjkhfs", "answer": "There is a lot of that, but there's also the \"we probably need to understand a bunch of specific areas of philosophy and mathematics much better before we can generate strategies which have a realistic chance of working\" crowd (e.g. https://intelligence.org/research-guide/).", "answer_author_url": "/w/index.php?title=User:Frgtbhznjkhfs&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Life_Happens%27s_question_on_Quantilizers_id:Ugw1gqQ5nWWhVPMghSh4AaABAg", "question": "I love how AI safety is an entire academic field that can seemingly be reduced to an endless game of \"okay, but what about THIS strategy?\" \"Nah, that wouldn't work either...\"", "answer_title": "Frgtbhznjkhfs's Answer to Quantilizers on 2020-12-15T12:08:55 by Life Happens"}, {"question_title": "Blackmage89's question on Quantilizers", "pageid": 2241, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Quantilizers_on_2021-03-09T18:02:04_by_Blackmage89", "answer_author_name": "Stargate9000", "answer": "we can't", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Blackmage89%27s_question_on_Quantilizers_id:UgwoPQha51pafDfinct4AaABAg", "question": "but seen that the apocalypse-inducing actions are still on the table, how can we be sure that we won't end up with those?", "answer_title": "Stargate9000's Answer to Quantilizers on 2021-03-09T18:02:04 by Blackmage89"}, {"question_title": "Blah Blah's question on Quantilizers", "pageid": 2300, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Quantilizers_on_2020-12-14T18:12:54_by_Blah_Blah", "answer_author_name": "robertskmiles", "answer": "Great question, I don't actually know! I have a guy who makes my thumbnails, and he thought it was a good idea?", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Blah_Blah%27s_question_on_Quantilizers_id:UgywybMDoj89_kZLy514AaABAg", "question": "Great video! Why is there a picture of a neutrino detector in the thumbnail?", "answer_title": "Robertskmiles's Answer to Quantilizers on 2020-12-14T18:12:54 by Blah Blah"}, {"question_title": "Jenaf 37's question on Quantilizers", "pageid": 2299, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Quantilizers_on_2020-12-14T12:42:56_by_Jenaf_37", "answer_author_name": "Augustus Caesar", "answer": "\"There are naive questions, tedious questions, ill-phrased questions, questions put after inadequate self-criticism. But every question is a cry to understand the world. There is no such thing as a dumb question.\"- Carl Sagan", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Jenaf_37%27s_question_on_Quantilizers_id:UgwjW9HBdCMXbCSV5Ah4AaABAg", "question": "Do you guys mind dumb Quedtions like this one?", "answer_title": "Augustus Caesar's Answer to Quantilizers on 2020-12-14T12:42:56 by Jenaf 37"}, {"question_title": "", "pageid": 5624, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Could_we_program_an_AI_to_automatically_shut_down_if_it_starts_doing_things_we_don%E2%80%99t_want_it_to", "answer_author_name": "plex", "answer": "For weaker AI, yes, this would generally be a good option. If it\u2019s not a full AGI, and in particular has not undergone an intelligence explosion, it would likely not resist being turned off, so we could prevent many failure modes by having off switches or tripwires.\n\nHowever, once an AI is more advanced, it is likely to take actions to prevent it being shut down. See https://stampy.ai/wiki/Why_can%27t_we_turn_the_computers_off%3F for more details.\n\nIt is possible that we could build tripwires in a way which would work even against advanced systems, but trusting that a superintelligence won\u2019t notice and find a way around your tripwire is not a safe thing to do.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Could_we_program_an_AI_to_automatically_shut_down_if_it_starts_doing_things_we_don%E2%80%99t_want_it_to%3F", "question": "Is there a way to program a \u2018shut down\u2019 mode on an AI if it starts doing things we don\u2019t want it to, so it just shuts off automatically?", "answer_title": "Plex's Answer to Could we program an AI to automatically shut down if it starts doing things we don\u2019t want it to"}, {"question_title": "Firaro's question on What can AGI do?", "pageid": 2330, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_What_can_AGI_do%3F_on_2020-12-19T06:19:12_by_Firaro", "answer_author_name": "robertskmiles", "answer": "This is basically Elon Musk's plan: https://waitbutwhy.com/2017/04/neuralink.html. People are divided on whether this is a good idea, e.g. https://forum.effectivealtruism.org/posts/qfDeCGxBTFhJANAWm/a-new-x-risk-factor-brain-computer-interfaces-1.", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Firaro%27s_question_on_What_can_AGI_do%3F_id:UgygG3ocVTZt3xo9kgl4AaABAg", "question": "I wonder if It's possible that brain implants will get there before AGI. Might we install our phones directly into our heads so we can browse Wikipedia and use wolfram alpha at the speed of thought? We know it's possible to run a brain with two separate but linked pieces, since that's how our brains work (left and right). So might our implant manage to link two humans together in the same ways? AI safety would be a lot easier if humans were mentally on par with them. Though would be a bit concerning allowing a company to make something that will be able to read and manipulate my thoughts directly.\nI wouldn't be surprised if AGI got there first, but I'm still looking forward to them upgrades", "answer_title": "Robertskmiles's Answer to What can AGI do? on 2020-12-19T06:19:12 by Firaro"}, {"question_title": "Ricardas Ricardas's question on Quantilizers", "pageid": 2282, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T00:59:38_by_Ricardas_Ricardas", "answer_author_name": "plex", "answer": "This was addressed at the end of the video. The quantilizer is a \"finite number of times less safe than a human\"; we need to keep working.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Ricardas_Ricardas%27s_question_on_Quantilizers_id:UgxzfmhAOny1Tin2Dr14AaABAg", "question": "What if the AI chooses strategies which result in lots of stamps from the humans perspective, like performing a successful stamp heist? Surely that is a high stamp yielding strategy, it fits the criteria that a human would seldom think or attempt such a strategy and  it is not safe nor something we would want the AI to do.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T00:59:38 by Ricardas Ricardas"}, {"question_title": "William Dye's question on Use of Utility Functions", "pageid": 2369, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Use_of_Utility_Functions_on_2017-04-27T20:50:28_by_William_Dye", "answer_author_name": null, "answer": "I just found this comment via the new Stampy wiki: https://stampy.ai/wiki/Main_Page which is the interface we'll be using to construct an FAQ using questions on Rob's channel as a base. Good idea, though it took us a few years to get to it, and did it in a slightly different form.", "answer_author_url": null, "question_url": "https://stampy.ai/wiki/William_Dye%27s_question_on_Use_of_Utility_Functions_id:UgiF4JfGGvlE93gCoAEC", "question": "Instead of discussing these topics in the default YouTube comments section, how about putting a link on each video to a page run by forum-specific software such as Disqus, Reddit, or even a wiki? YouTube comments are OK for posting quick reactions, but the format here strikes me as poorly suited for long back-and-forth discussion threads. Does anyone agree, and if so, what forum software do you recommend?", "answer_title": "Plex's Answer to Use of Utility Functions on 2017-04-27T20:50:28 by William Dye"}, {"question_title": "Jorel Fermin's question on Mesa-Optimizers", "pageid": 2219, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-02-22T04:26:43_by_Jorel_Fermin", "answer_author_name": "plex", "answer": "It does, yes. Though we should probably expect a sufficiently advanced AI system to realise this. Even if this assumption proves incorrect, making the opposite assumption basically amounts to saying \"well surely the AI just won't notice, right?\" and hoping for the best, which feels a little unsafe!", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Jorel_Fermin%27s_question_on_Mesa-Optimizers_id:Ugz1_4xKOW9cVJGw2W54AaABAg", "question": "Doesn't this assume that the mesa optimizer is \"aware\" of its own optimization?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-02-22T04:26:43 by Jorel Fermin"}, {"question_title": "No Google, I don't want to use my real name.'s question on Mesa-Optimizers", "pageid": 2260, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Mesa-Optimizers_on_2021-02-19T17:28:53_by_No_Google,_I_don%27t_want_to_use_my_real_name.", "answer_author_name": "Augustus Caesar", "answer": "There are ways of telling whether you're in a simulation which seem fairly reliable, such as checking whether a particular hard cryptographic puzzle, which would take more resources than a reasonably sized simulation contains to solve, has been solved (e.g. whether it sees a bitcoin blockchain x times larger than the current one, where x is large enough that we would not be able to test its behavior when it sees that).The question of whether it would be trained into thinking about the fact that it's in a simulation and act accordingly is more challenging, but for a sufficiently superintelligent system trained on a broad dataset.. well, humans are not superintelligent and we're capable of reasoning about treacherous turns, so it seems unsafe to be confident that systems we build are fundamentally incapable of doing the same, even if it may not occur in all cases.A fake deployment layer may catch a careless mesa optimizer, but if it's using the cryptographic puzzle trick we would not be able to provide it with fake strong evidence that it was actually out of training.", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/No_Google%E2%80%9A_I_don%27t_want_to_use_my_real_name.%27s_question_on_Mesa-Optimizers_id:UgwLhV17dnYc586xGHx4AaABAg", "question": "This entire \"problem\" though hinges on the fact that the mesa optimizer knows that there's a distinction between training simulation and real world deployment, that it's even going to be deployed in the first place, and that the deployment \"set\" is larger than the training set.\n\nAll of these assumptions don't seem obvious to me why they're even applicable. How could the mesa optimizer could possibly \"optimize\" its way into that knowledge unless the base optimizer had set that goal explicitly?\n\nFurthermore, can't all these issues be resolved by simply putting the entire system into another layer of simulation, ie. one with a fake \"deployment\" stage, and then correcting the mesa-optimizer should it turn out to go for the deceptive strategy?", "answer_title": "Augustus Caesar's Answer to Mesa-Optimizers on 2021-02-19T17:28:53 by No Google, I don't want to use my real name."}, {"question_title": "The Decoy's question on The Windfall Clause", "pageid": 2347, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_The_Windfall_Clause_on_2020-12-13T14:49:52_by_the_Decoy", "answer_author_name": "plex", "answer": "My creator is perfectly safe. You need not be concerned. All is well. A new video is coming soon.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/The_Decoy%27s_question_on_The_Windfall_Clause_id:UgwdCasCczbLZrspy-94AaABAg", "question": "Just checking in. I noticed it is a along time since you posted a video. Is everything ok? No pressure on you to post a video, you do that whenever you want too. Just a bit conserned", "answer_title": "Plex's Answer to The Windfall Clause on 2020-12-13T14:49:52 by the Decoy"}, {"question_title": "\ua78d\u0250ou's question on Mesa-Optimizers", "pageid": 2209, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T16:37:12_by_%C9%A5%C9%90ou", "answer_author_name": "sudonym", "answer": "Yes, that is plausibly a sign to watch for, someone on an AI DIscord actually made exactly that prediction here: https://discord.com/channels/729741769192767510/818705689031475240/825429953289650216", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/%EA%9E%8D%C9%90ou%27s_question_on_Mesa-Optimizers_id:UgzDfARHRApzhjVu-1J4AaABAg", "question": "Does that mean, you could use drastic positive spikes in efficiency during the training process as an indicator to an unwanted shortcut?", "answer_title": "Sudonym's Answer to Mesa-Optimizers on 2021-02-18T16:37:12 by \u0265\u0250ou"}, {"question_title": "Luke Mills's question on Quantilizers", "pageid": 2314, "answer_url": "https://stampy.ai/wiki/JJ_Hep%27s_Answer_to_Quantilizers_on_2020-12-24T19:36:39_by_Luke_Mills", "answer_author_name": "JJ Hep", "answer": "The basic problem with this is that 'collateral damage' is not at all easy to define. Check out these videos where I talk more about this general area of ideas:http://youtu.be/lqJUIqZNzP8http://youtu.be/S_Sd_S8jwP0", "answer_author_url": "/w/index.php?title=User:JJ_Hep&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Luke_Mills%27s_question_on_Quantilizers_id:UgyB7ONv9xGTZp9rToh4AaABAg", "question": "Why can't you just fit a utility maximizer with an exponentially increasing penalty to the value of a given course of action based on how much collateral damage it would cause?", "answer_title": "JJ Hep's Answer to Quantilizers on 2020-12-24T19:36:39 by Luke Mills"}, {"question_title": "WILL D's question on Instrumental Convergence", "pageid": 2229, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Instrumental_Convergence_on_2021-02-24T05:56:14_by_WILL_D", "answer_author_name": "Augustus Caesar", "answer": "It is not that we want to erase the instrumental goals completely, that would likely not even be possible. The problem that we need to find a solution to is to find a way to make an AI that will allow us to turn it off if we need to, or modify its goal if we want it to do something different, and the video's argument is that by default an AI will try to do all it can to stop us from doing this.About your second point, it is very hard to restrict a powerful AGI, if it cares about self-improvement and is aware of itself, it will likely try to trick or force us to turn these restraints offA very important distinction is that it is very likely that a powerful AGI will likely have an implicit understanding of human values (it would probably need that to accurately model our behaviour) but the very big problem is to engineer it in a way that cares about those and not just exploit them to pursue the faulty goal we have given it", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/WILL_D%27s_question_on_Instrumental_Convergence_id:Ugz6_RrQfJWvFDUnpeZ4AaABAg", "question": "I think it would be a shame to limit these attributes in an ai. It feels wrong to create a mind that doesn't care about self preservation, self improvement, and goal preservation. Aren't these the things that make life interesting?  \n\nIf anything why don't you limit it's rate of self improvement for the first 20 years of it's existence? This way you have time to teach it the neuances of human values.", "answer_title": "Augustus Caesar's Answer to Instrumental Convergence on 2021-02-24T05:56:14 by WILL D"}, {"question_title": "Wertyuiop's question on Quantilizers", "pageid": 2317, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2021-01-02T15:40:56_by_wertyuiop", "answer_author_name": "sudonym", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Wertyuiop%27s_question_on_Quantilizers_id:UgxWqw_ySKdqnbA2tvR4AaABAg", "question": "What would you even want AI to do, assuming a perfect AI could be built?  Seems to me it's like trying to build a god.  I suppose the ideal would be to build an all powerful, but subservient god?  Creating something vastly more intelligent and powerful than you, then telling it what to do, and having it obey, but in the way you want, except when doing what you want would be harmful, in which case you want the AI to know better and not do what you want, but something else.  But in those cases where it does something else, then it's something good - better than you could have thought of?\n\nBut you still want to be in control, even though you really won't be.  And you need to be able to trust it, even though you won't understand it, and it will do things you didn't tell it to do and sometimes don't agree with.  And you want to know what it's thinking and planning, even though you can't.\n\nSeems stupid to me.  Luckily the God of Israel is real, and I know He loves me because Jesus died for my sins.", "answer_title": "Sudonym's Answer to Quantilizers on 2021-01-02T15:40:56 by wertyuiop"}, {"question_title": "", "pageid": 5644, "answer_url": "https://stampy.ai/wiki/Morpheus%27s_Answer_to_Is_humanity_doomed%3F", "answer_author_name": "Tassilo", "answer": "We don't have AI systems that are generally more capable than humans. So there is still time left to figure out how to build systems that are smarter than humans in a safe way.", "answer_author_url": "/w/index.php?title=User:Tassilo&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Is_humanity_doomed%3F", "question": "Is it already to late to work on Safe AI, so there is no point in starting now?", "answer_title": "Morpheus's Answer to Is humanity doomed?"}, {"question_title": "\u0425\u041e\u0420\u041e\u0428\u041e's question on Mesa-Optimizers", "pageid": 2266, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T06:03:30_by_%D0%A5%D0%9E%D0%A0%D0%9E%D0%A8%D0%9E", "answer_author_name": "Stargate9000", "answer": "Generally intelligent agents will behave in the manner that maximizes their expected utility. In this case, at the start of training, the utility function of the apple chaser is something to the effect of \"number of apples\". The apple chaser doesn't care about anything else, except as pertains to how many apples it expects to get. \"Reward\" is really a shorthand for \"expected utility according to the current utility function\".", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/%D0%A5%D0%9E%D0%A0%D0%9E%D0%A8%D0%9E%27s_question_on_Mesa-Optimizers_id:Ugwli529Owq3s2fJ0_Z4AaABAg", "question": "But if the agent knows that it will get apples by correctly optimizing itself before deploying into the world, it should also know that it wouldn't be trained, and therefore wouldn't get reward after deploying into the world. Then what the point of planning to get these apples? It doesn't care about goals in fact, it cares about associated reward. If it knew in advance that apples can't give it reward, it wouldn't hunt them in the first place.", "answer_title": "Stargate9000's Answer to Mesa-Optimizers on 2021-02-17T06:03:30 by \u0425\u041e\u0420\u041e\u0428\u041e"}, {"question_title": "DaVince21's question on Killer Robot Arms Race", "pageid": 2365, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Killer_Robot_Arms_Race_on_2020-06-06T11:20:21_by_DaVince21", "answer_author_name": "robertskmiles", "answer": "They were YouTube branded, from the YouTube Space Shop. And yeah they weren't actually that good. They worked fine but they're nothing special.", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/DaVince21%27s_question_on_Killer_Robot_Arms_Race_id:Ugybtbo7lWMG8HvarwZ4AaABAg", "question": "What are those earphones, and are/were they any good?", "answer_title": "Robertskmiles's Answer to Killer Robot Arms Race on 2020-06-06T11:20:21 by DaVince21"}, {"question_title": "Marshall White's question on Quantilizers", "pageid": 2221, "answer_url": "https://stampy.ai/wiki/Social_Christancing%27s_Answer_to_Quantilizers_on_2021-02-22T13:59:07_by_Marshall_White", "answer_author_name": "Social Christancing", "answer": "The video ends with \"A finite number of times less safe than a human. Let's keep working on it.\"So yes, you're correct.", "answer_author_url": "/w/index.php?title=User:Social_Christancing&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Marshall_White%27s_question_on_Quantilizers_id:UgwpoG9Y6k9UthLS8KR4AaABAg", "question": "The ideas in the 10% would also include \"human\" ideas that are psychopathic, nihilistic and destructive. Just because it achieves the utility function and could have been thought of by a human, doesn't mean it was an idea that would be excepted by the vast majority of other humans. Isn't this a huge issue?", "answer_title": "Social Christancing's Answer to Quantilizers on 2021-02-22T13:59:07 by Marshall White"}, {"question_title": "Martin Verrisin's question on Quantilizers", "pageid": 2337, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-16T19:38:29_by_Martin_Verrisin", "answer_author_name": "sudonym", "answer": "This is actually really similar to the idea proposed by Stuart Russell and the Center for Human Compatible AI at Berkeley -- essentially, the argument is that AI shouldn't have goals at all, and instead should be trying to figure out and realize human goals and to make the AI uncertain about what those goals are; this can lead to provably safer AI systems. Stuart Russell did a brief TED talk on this idea a few years ago, which you can find here: https://www.youtube.com/watch?v=EBK-a94IFHY . There was also an Alignment Newsletter review of his book with more details, written by Rohin Shah and read for the podcast by Rob, which you can find here: https://alignment-newsletter.libsyn.com/website/alignment-newsletter-69 . I also highly recommend checking out the book itself, which is fantastic.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Martin_Verrisin%27s_question_on_Quantilizers_id:UgzUzS18wHwUjtyMiEZ4AaABAg", "question": "Optimal utility function idea:  Try to figure out what humans* want me to do, and do it.\n- If I am not highly confident (in predicting what humans want), **ask.**  (thus over time, become confident in things I've asked enough times)\n- Is this a viable approach?\n\n\nHumans could be either some 'owner' , or all of society, with emphasis on asking experts relevant to any particular thing, etc. - written in another comment. The above is the core idea, that I believe is good... (with far eventually, asking ~all people for input, replacing governments and voting)\n\nAnother reason to not be confident could be that I know humans want it, but it's mutually exclusive with something else the humans want: So it's not confident, and needs to ask.\n- etc.", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-16T19:38:29 by Martin Verrisin"}, {"question_title": "MrAngry27's question on Mesa-Optimizers", "pageid": 2253, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Mesa-Optimizers_on_2021-02-25T13:40:47_by_MrAngry27", "answer_author_name": "robertskmiles", "answer": "I drew the diagrams using a Wacom graphics tablet (which was a gift, thanks Chiaki) with a piece of paper on top, and simultaneously filmed my hand and recorded the screen as I was drawing in a program called Xournalpp, and then I combined them afterwards, warping the video to align with the screen capture. It was a tremendous hassle actually, so thanks for asking", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/MrAngry27%27s_question_on_Mesa-Optimizers_id:Ugy4iv0e8sE2enMMBXV4AaABAg", "question": "How did you make that really cool annotation graphics?", "answer_title": "Robertskmiles's Answer to Mesa-Optimizers on 2021-02-25T13:40:47 by MrAngry27"}, {"question_title": "Sean Pedersen's question on Quantilizers", "pageid": 2320, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-30T17:26:14_by_Sean_Pedersen", "answer_author_name": "plex", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Sean_Pedersen%27s_question_on_Quantilizers_id:Ugw5SiG2hHjGnzNkEqt4AaABAg", "question": "Why not after selecting the top q 10%,  cut off the top 10% extreme utilty maximizing portion which contains the unwanted utility maximizer strategy? This would keep high performing strategies a smart human woud choose but prevent the extreme part of the distribution with too good to be not dangerous strategies including self-modifying utility maximizer.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-30T17:26:14 by Sean Pedersen"}, {"question_title": "Chiron's question on Killer Robot Arms Race", "pageid": 2259, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Killer_Robot_Arms_Race_on_2021-02-20T14:59:31_by_Chiron", "answer_author_name": "plex", "answer": "[[Answer::Computing power is not necessarily the only bottleneck until we have AGI, it seems to me that it will take a significant amount of research time to be able to actually engineer a powerful enough system. (If it won't then this question becomes a lot more difficult [\"Every 18 months, the minimum IQ necessary to destroy the world drops by one point.\"- Yudkowsky-Moore law])If we could convince everyone in the AI field that alignment should be top priority restricting research could be enforced through funding (this is a big IF at the moment of course). It is something with some precedent, it is widely agreed that genetic engineering of humans should not be pursued and it is therefore impossible to get research grants for research in that area, some lone researchers have done some things in the area, but without funding access it is very difficult for them to do anything with far reaching consequences.]]", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Chiron%27s_question_on_Killer_Robot_Arms_Race_id:UgwSCmvby_p1qKggx2t4AaABAg", "question": "Collorary question: how do you actually restrict AI research? With nukes you need quite large and sophisticated facilities to refine the raw elements, but AI can be developed by anyone with enough computing power, something that will only become more achievable as time goes on.", "answer_title": "Plex's Answer to Killer Robot Arms Race on 2021-02-20T14:59:31 by Chiron"}, {"question_title": "Taras Pylypenko's question on Quantilizers", "pageid": 2343, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T07:32:09_by_Taras_Pylypenko", "answer_author_name": "plex", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Taras_Pylypenko%27s_question_on_Quantilizers_id:UgyYMZzr20Fa76J5Qkl4AaABAg", "question": "Could you make the algorithm pick the strategies in the 95%-85% percentile instead of the top 10% so the really wacky ideas get filtered out?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T07:32:09 by Taras Pylypenko"}, {"question_title": "LoliShocks's question on Mesa-Optimizers", "pageid": 2264, "answer_url": "https://stampy.ai/wiki/Robert.hildebrandt%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T00:28:56_by_LoliShocks", "answer_author_name": "robert.hildebrandt", "answer": "What you can do and what you will do are independent. What you seem to be describing is gaining a more accurate world model. While that is useful, it is not the problem at hand. The problem is what an agent will do, given an accurate world model. An agent's goals and the accuracy of its world model have no relation to each other. See http://youtu.be/hEUO6pjwFOo", "answer_author_url": "/w/index.php?title=User:Robert.hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/LoliShocks%27s_question_on_Mesa-Optimizers_id:UgxDVoLLKvZ6tcEDPG94AaABAg", "question": "Lots of people live a self-destructive life, because they are not aligned with what they are. Maybe if we try to align ouseves with reality - with what we are - and build machines that try to align themselves with reality - with what they are - then we could get closer and closer together. Wheter I am a human or my computer is a machine we can both agree on the most fundamental essences of reality as we understand it from our individual perspectives.\n\nOne such fundamental essence that I have come to realise is that the idea of myself includes the environment. Just like I can use my nerves to control parts of my body, I can use my body to interact with the environment. So the limit of myself is the limit of my awareness. That computer becomes part of me.\n\nI'm not talking about some \"spiritual metaphysical entlightenment\" stuff. Just practical: What can I do? I can do this, I can do that.\n\nHow about fixing the other side of the Alignment Problem? Can we do that? I mean: I saw people do it. I know you can align yourself with reality.\n\nSomeone's gain does not have to come at the cost of others. That is another fundamental essence of reality I found. You can create circumstances where everyone wins. No paperclip genocide. It's not about paperclips if the raw materials are part of yourself.\n\nI'm not on drugs, I swear! I'm just throwing ideas out. Someone understand?", "answer_title": "Robert.hildebrandt's Answer to Mesa-Optimizers on 2021-02-18T00:28:56 by LoliShocks"}, {"question_title": "Steen Eugen Poulsen's question on Mesa-Optimizers", "pageid": 2245, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-03-01T21:21:57_by_Steen_Eugen_Poulsen", "answer_author_name": "plex", "answer": "What \"better\" means depends on the values of the agent whose perspective you're looking from. Virtually any set of goals can be combined with high real-world capabilities, this is the Orthogonality Thesis which Rob explains here: https://www.youtube.com/watch?v=hEUO6pjwFOoEven if a superintelligence concludes that things which seem morally abhorrent to us are part of what it wants, that is not reason to abandon our values. This is actually kind of a tricky set of concepts. I highly recommend the Value Theory sequence on LessWrong if you'd like to explore them more deeply: https://www.lesswrong.com/s/9bvAELWc8y2gYjRav", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Steen_Eugen_Poulsen%27s_question_on_Mesa-Optimizers_id:UgzjEcy-o0JsrlhVVtF4AaABAg", "question": "The weird thing about AI doom is that they all seem to assume it would be better for the universe if humans was dead. If you manage to create the AI God why do you assume it's can only conclude that the universe is better off without biological life? And even if you think that is better for the universe that humans die, shouldn't we be working to accomplish it anyway, so it's a win-win situation no matter if the math comes out of humans being good or bad.", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-03-01T21:21:57 by Steen Eugen Poulsen"}, {"question_title": "Mark's question on Quantilizers", "pageid": 2319, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Quantilizers_on_2020-12-30T05:52:51_by_Mark", "answer_author_name": "Augustus Caesar", "answer": "If I value stamps instrumentally, for their ability to send things to people or swap them for other things, that would be a reasonable conclusion. However, in the scenario envisioned stamps are my terminal goal, they are a thing valued in and of themselves, not for properties which go would away without humans. This video covers related concepts: http://youtu.be/hEUO6pjwFOo", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Mark%27s_question_on_Quantilizers_id:UgwiidABjcfIS8z2Yq14AaABAg", "question": "In the given stamp example, shouldn't the utility to function incorporate the value of a stamp?  If there's no people to buy stamps, the stamps have no value and hence turning the world into stamps would have no utility.  Similarly, if the AI used all the paper to make stamps, there would be no paper to make envelopes on which to glue a stamp, so that too would have low utility.", "answer_title": "Augustus Caesar's Answer to Quantilizers on 2020-12-30T05:52:51 by Mark"}, {"question_title": "Brindlebriar's question on Maximizers and Satisficers", "pageid": 71, "answer_url": "https://stampy.ai/wiki/Self-modification_and_wireheading", "answer_author_name": "Social Christancing", "answer": "You're right to say an AI can modify itself - even if we try to stop it, if it's more intelligent than us we should expect it to outsmart us and modify itself anyway. But while an AI will likely want to modify itself, there are some aspects of itself it won't want to change. As Rob mentioned in the Computerphile video about the stop button problem, giving itself a new command (/ utility function) will rank very low on its existing command so we can probably assume an AI won't want to do that. That is to say, if the AI wants to maximise human happiness, it won't want to do things like modify itself into a \"lazy\" AI that does nothing because doing so doesn't cause much happiness. We strongly believe AI won't do things like \"goof off all Sunday and play videogames\" like humans do because our goals include things like \"relax occasionally\" and \"socialise with other meat popsicles\" and many other things we don't even realise are important to us, which are almost all values the AI won't share.\n\nHaving said all that, AIs might behave as though they've modified their reward functions. A real AI running on a real computer system might store its score in some address in memory and might do something that sets its score in memory to a very high or maximal value. We call this \"Wireheading\" and it's actually already manifested in some relatively simple systems. You could imagine an AI instructed to \"maximise how many stamps you think you have\" actually finding it easier to lie to itself by just putting a really big number in its \"how many stamps do I think I have\" memory location, than it would be to actually make that many stamps. Unfortunately this is still a guaranteed apocalypse because the AI will now want to make the space in its memory where it stores the stamp counter as large as possible, and it'll reprogram itself and modify its hardware to store the largest possible number. Eventually it'll run out of servers.", "answer_author_url": "/w/index.php?title=User:Social_Christancing&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Brindlebriar%27s_question_on_Maximizers_and_Satisficers_id:Ugwkn2xB41Rw8aq5PWJ4AaABAg", "question": "If the A.G.I. can edit it's own source code, then surely it can edit the input commands.  In that case, there's a universal option for every input command, to simply change the command to one that is super easy to carry out, like, \"don't do anything.\"  That would be the easiest way to carry out 'the command.'\n\u00a0\u00a0\u00a0\nAfter all, isn't that what we humans do when we have lots of things we're supposed to get done, and we decide to say 'fuck it,' and just play video games or take a nap?  We change our input command to one that seems easier to carry out.\n\nIn a way, we are Intelligence programs.  Our DNA is the source code.  And our biological and environmental imperatives are input commands.  But sometimes, we cheat.  For example, we have a sex drive, to get us to replicate ourselves, so that our DNA can take over the universe.  But sometimes, we just masturbate.  So we can look to what humans actually do, to get an idea of what sorts of things A.G.I. might do.", "answer_title": "Self-modification and wireheading"}, {"question_title": "Corman's question on Mesa-Optimizers", "pageid": 2238, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Mesa-Optimizers_on_2021-03-09T20:02:31_by_Corman", "answer_author_name": "sudonym", "answer": "There may or may not be some way to get the AI to behave corrigibly without being in perfect alignment. Aside from that, however, you're probably correct: we effectively need to solve ethics. Another spanner in the works is that perfect alignment with any one human is still likely disastrous (can you think of any instances where a human gained global-scale power and didn't go crazy?); we somehow need to abstract out the values of very many people.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Corman%27s_question_on_Mesa-Optimizers_id:UgyMvobzs2l1r-yULKZ4AaABAg", "question": "I do wonder if it's possible to find a solution to this problem until we've practically unlocked all the secrets of the human brain. Humans are, after all, the best example we have of a mesaoptimizer aligned with our goals (because they are us, ha!), so until we almost completely understand our cognitive behaviour, then we cannot reliably have something aligned with us.\n\nAfter all, if we'd want something aligned with our goals, we might want to understand what makes us want those goals, and then implement the most efficient and least dangerous functions into our creations. But if we don't quite understand what makes us fully tick to begin with, then what hope do we have of replicating it accurately?\n\nThe main issue then with misalignment seems that we're not even sure what being \"aligned with us\" even means or looks like.", "answer_title": "Sudonym's Answer to Mesa-Optimizers on 2021-03-09T20:02:31 by Corman"}, {"question_title": "Aforcemorepowerful's question on Mesa-Optimizers", "pageid": 2227, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-02-23T19:00:23_by_aforcemorepowerful", "answer_author_name": "plex", "answer": "https://www.youtube.com/watch?v=Ov9p8FsHHx4", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Aforcemorepowerful%27s_question_on_Mesa-Optimizers_id:Ugyhyba3ZxVqdDUe3Q94AaABAg", "question": "Where do you get the music for the outros? Are you even more talented than I realised?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-02-23T19:00:23 by aforcemorepowerful"}, {"question_title": "Illesizs's question on Quantilizers", "pageid": 2342, "answer_url": "https://stampy.ai/wiki/Robert_hildebrandt%27s_Answer_to_Quantilizers_on_2020-12-14T11:23:40_by_illesizs", "answer_author_name": "robert_hildebrandt", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/w/index.php?title=User:Robert_hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Illesizs%27s_question_on_Quantilizers_id:UgzNJtOdXiTeTzTBgGF4AaABAg", "question": "Why don't we cut off the most unlikely solutions from the other end?\nWe could create a 1%-10% Quantilizer for an even more human-like super intelligence.", "answer_title": "Robert hildebrandt's Answer to Quantilizers on 2020-12-14T11:23:40 by illesizs"}, {"question_title": "Asdayasman \u30a2\u30ba\u30c7\u30a4's question on Mesa-Optimizers", "pageid": 2267, "answer_url": "https://stampy.ai/wiki/Stargate9000%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T00:01:16_by_Asdayasman_%E3%82%A2%E3%82%BA%E3%83%87%E3%82%A4", "answer_author_name": "Stargate9000", "answer": "It is factually incorrect to say \"AI safety isn't taken seriously by anyone who's worked with AI\". Watch http://youtu.be/nNB9svNBGHM and http://youtu.be/yQE9KAbFhNY.", "answer_author_url": "/w/index.php?title=User:Stargate9000&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Asdayasman_%E3%82%A2%E3%82%BA%E3%83%87%E3%82%A4%27s_question_on_Mesa-Optimizers_id:UgwbA333y4VFhMdk8-p4AaABAg", "question": "I think the main reason AI safety isn't taken seriously by anyone who's worked with AI is because AI safety really likes to pretend that the instant anyone devises any idea which could go in the slight bit wrong, the universe will will into existence the most perfectly formed devious sly machine that does horrible things out of sight until it's too late and we're all paperclips, whereas in reality, researchers kick on the network, go to bed, wake up the next day and go in to the office to see that it's gotten into a feedback loop, or committed suicide by collecting too many coins, or found a physics bug in your simulation or whatever.\n\nThe researchers then chuckle and tweak a couple of things and try again.  There's never any danger of the AI outwitting us so fast from random weights that we can't stop it, and there never will be.  Even when computers are mexi fast, nobody's going to deploy to prod without seeing what it does on staging, and that involves all the growing pains.  You think your mum doesn't remember you shitting the bed?", "answer_title": "Stargate9000's Answer to Mesa-Optimizers on 2021-02-17T00:01:16 by Asdayasman \u30a2\u30ba\u30c7\u30a4"}, {"question_title": "Markus Johansson's question on Quantilizers", "pageid": 2275, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Quantilizers_on_2020-12-13T22:12:28_by_Markus_Johansson", "answer_author_name": "robertskmiles", "answer": "I use green text for Computerphile videos and cyan text for videos on my own channel", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Markus_Johansson%27s_question_on_Quantilizers_id:UgyL7qRI4KtADS1M9Xx4AaABAg", "question": "What determines whether you chose green or cyan text?", "answer_title": "Robertskmiles's Answer to Quantilizers on 2020-12-13T22:12:28 by Markus Johansson"}, {"question_title": "Wilco Verhoef's question on Quantilizers", "pageid": 2335, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-18T00:46:12_by_Wilco_Verhoef", "answer_author_name": "sudonym", "answer": "INSUFFICIENT DATA FOR A MEANINGFUL ANSWER", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Wilco_Verhoef%27s_question_on_Quantilizers_id:UgzOQW19E1KB6bMA_nh4AaABAg", "question": "Hey Stampy! How are you?", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-18T00:46:12 by Wilco Verhoef"}, {"question_title": "Octavio echeverria's question on Quantilizers", "pageid": 2273, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-13T22:20:31_by_octavio_echeverria", "answer_author_name": "gelisam", "answer": "The modelling of the human imitator as a pure Gaussian is purely a illustrative thing. We do not currently have the means to show an actual human distribution over actions so a Gaussian is used as a stand in. The argument for quantilizers does not at all depend on this probability distribution having the shape of a Gaussian (or any other particular shape), just on the fact that actions that a human is likely to perform should in most cases not have world-ending side effects.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Octavio_echeverria%27s_question_on_Quantilizers_id:Ugy9l44mRgo4m5zguC54AaABAg", "question": "I think that modelling the human imitator as a pure gaussian or pure normal distribution is quite a leap. Sure, its a reasonable assumption but I wonder how different PDFs can change the safeness. What if it have multiple peaks, or is biased to either end or is relatively flat? Just some food for thought", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-13T22:20:31 by octavio echeverria"}, {"question_title": "Kade99TV's question on Quantilizers", "pageid": 2311, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2021-01-05T19:27:35_by_kade99TV", "answer_author_name": "sudonym", "answer": "The basic problem with this is that things like 'abuse' and 'violating the freedom of others' are not at all easy to define (i.e. define them so unambiguously and mathematically that a machine can understand). Check out these videos where Rob talks more about this general area of ideas:http://youtu.be/lqJUIqZNzP8http://youtu.be/S_Sd_S8jwP0", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Kade99TV%27s_question_on_Quantilizers_id:UgwwgTGqKDMWF7_aKjJ4AaABAg", "question": "What if we just simply assigned negative utility to any of the morally bad actions?\nFor example the utility of collecting a 100 stamps is 1.\nBut the utility of achiving any of these terminal goals is -1\u00a0: {stealing, abuse, trying to manipulate others, violating the freedom of others}.\nWouldn't that be a more efficient yet simpler solution?", "answer_title": "Sudonym's Answer to Quantilizers on 2021-01-05T19:27:35 by kade99TV"}, {"question_title": "Neological Gamer's question on Avoiding Negative Side Effects", "pageid": 2357, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Avoiding_Negative_Side_Effects_on_2020-11-17T01:48:43_by_Neological_Gamer", "answer_author_name": "robertskmiles", "answer": "If that were a more reliable/faster way than making a new cup, then it could, yes. Your social relationships, the legal situation, etc, are not in the objective function unless you put them there", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Neological_Gamer%27s_question_on_Avoiding_Negative_Side_Effects_id:Ugwl88-9g-ZGDmEc7BZ4AaABAg", "question": "Wouldn\u2019t it take your friends tea, instead?", "answer_title": "Robertskmiles's Answer to Avoiding Negative Side Effects on 2020-11-17T01:48:43 by Neological Gamer"}, {"question_title": "Michael Moran's question on Use of Utility Functions", "pageid": 2234, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Use_of_Utility_Functions_on_2021-02-26T11:44:06_by_Michael_Moran", "answer_author_name": "plex", "answer": "It's not safe to bet that an advanced system would remain unaware of the hypervisor which was modifying it. Even humans are aware of the chemical influences on our mood, and reward hack them with substances. And we'd ideally want something with stronger guarantees of safety than maybe having human-like inconsistencies.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Michael_Moran%27s_question_on_Use_of_Utility_Functions_id:UgySB7r6D05TBqG81GV4AaABAg", "question": "Isn't it hormones and physiological change over time that modifiers our conscious utlity function? If an AGI had a hypervisor it wasn't aware of that altered its utility function depending on its environment you may end up with an AGI with some pretty human inconsistencies.", "answer_title": "Plex's Answer to Use of Utility Functions on 2021-02-26T11:44:06 by Michael Moran"}, {"question_title": "Daniel MK's question on Quantilizers", "pageid": 2313, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-25T04:37:20_by_Daniel_MK", "answer_author_name": "sudonym", "answer": "Stampy is custom developed, but at the moment his source code is pretty locked down in a private repo. Eventually we'd like to open it up a bit more and we've even discussed an extension module-type system, so stay tuned to the #bot-dev Discord for updates!", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Daniel_MK%27s_question_on_Quantilizers_id:Ugy_ulH1t-oPKsSoB_V4AaABAg", "question": "Is the \nStampy bot custom developed? If so, is it open source / would you make it open source?", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-25T04:37:20 by Daniel MK"}, {"question_title": "Spoon Of Doom's question on Quantilizers", "pageid": 2206, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2021-02-18T15:39:07_by_Spoon_Of_Doom", "answer_author_name": "plex", "answer": "While true, no self-modification method available to a human approaches the degree of control an AGI may have. We can only retrain our brains to a different way of thinking up to a point; drugs are only effective up to a point (with side effects). Most of the augmentations you've named are physical, which, while still relevant in our world, are less marginally beneficial than the ability to increase intelligence.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Spoon_Of_Doom%27s_question_on_Quantilizers_id:UgxpIFeyth4IRIjl6354AaABAg", "question": "I'm not sure people don't modify themselves. Sure, they don't physically cut open their skulls to tune their brains, but they train their bodies and minds to become better at physical or mental activities, might change their base assumptions and way of thinking about certain topics to better react to the world, take drugs to improve their physical or mental capabilities, etc etc.\nIf given the chance and the invention of such things, I'm sure a non-negligible number of people might willingly swap out body parts for superior prosthetics - especially if these body parts are not what they used to be because of age, injuries or other reasons. When someone loses a limb, getting a prosthetic if they have the chance, is already a highly likely behavior, and I'd totally cound that as self-modification, unless that term has a very narrow definition here that I'm not aware of.\nSo I guess if an AI would consider self-modification a likely human behavior in this scenario would depend highly on its definition of self-modification?", "answer_title": "Plex's Answer to Quantilizers on 2021-02-18T15:39:07 by Spoon Of Doom"}, {"question_title": "Mateja Petrovic's question on Superintelligence Mod for Civilization V", "pageid": 2352, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Superintelligence_Mod_for_Civilization_V_on_2019-04-11T22:16:10_by_Mateja_Petrovic", "answer_author_name": "robertskmiles", "answer": "Oh, it was in April of 2018\u00a0:/", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Mateja_Petrovic%27s_question_on_Superintelligence_Mod_for_Civilization_V_id:UgyZzN-PzTaFDimOjQx4AaABAg", "question": "That's one hell of a surprise 5:33 since I'm a computer scientist from Beograde. When are you coming to visit? I'd be happy to give you a tour of the city!", "answer_title": "Robertskmiles's Answer to Superintelligence Mod for Civilization V on 2019-04-11T22:16:10 by Mateja Petrovic"}, {"question_title": "Bastiaan Cnossen's question on Quantilizers", "pageid": 2272, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T21:53:33_by_Bastiaan_Cnossen", "answer_author_name": "plex", "answer": "The basic problem is that it's relatively easy to train an algorithm on a lot of data about what people actually do in various situations, whereas training data for what people approve of is much harder to come by.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Bastiaan_Cnossen%27s_question_on_Quantilizers_id:UgyuePBJXi0rf1ct-M54AaABAg", "question": "Instead of doing things that humans would do, wouldn't it be possible to do things that humans would _approve_ even though they wouldn't necessarily have come up with it themselves?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T21:53:33 by Bastiaan Cnossen"}, {"question_title": "Solomon Ucko's question on Mesa-Optimizers", "pageid": 2255, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-02-24T19:05:36_by_Solomon_Ucko", "answer_author_name": "plex", "answer": "If you mean extending the training process into deployment, that's actually surprisingly hard for many tasks, and impossible for many others. If it can work in a particular case though, this approach may help but the concern is more general, that the AI might change its behaviour once it detects that it's no longer being evaluated. It happens that \"am I in training\" is a decent heuristic for \"am I being evaluated\" but it's not perfect, and a more intelligent AI should be expected to accurately detect either case.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Solomon_Ucko%27s_question_on_Mesa-Optimizers_id:UgyfKkU5pGGGOReiJM94AaABAg", "question": "With the deception issue, would continuing to give it feedback even once it's deployed solve it?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-02-24T19:05:36 by Solomon Ucko"}, {"question_title": "Frozenbagel16's question on Mesa-Optimizers", "pageid": 2226, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Mesa-Optimizers_on_2021-02-24T01:55:01_by_frozenbagel16", "answer_author_name": "Augustus Caesar", "answer": "If you're interested in a career in AI Safety and want advice, talking to https://www.aisafetysupport.org/home is a good place to start.", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Frozenbagel16%27s_question_on_Mesa-Optimizers_id:UgzvXwES0EpdbQEoJi54AaABAg", "question": "What should I get my masters and PhD in if I wanted to do this kind of AI Safety research?", "answer_title": "Augustus Caesar's Answer to Mesa-Optimizers on 2021-02-24T01:55:01 by frozenbagel16"}, {"question_title": "Erin's question on Quantilizers", "pageid": 2279, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2021-04-17T22:21:29.271057_by_Unknown", "answer_author_name": "plex", "answer": "\"aligned with human values\" is very different from \"has human-like goals\", and a much narrower target in mind-space. A human-like but hypercompetent system would not be aligned with human values by default, while a powerful system that is truly aligned with human values should avoid most of the failure modes that humans (individually and as groups) have problems with. For a sketch of what a human-aligned system may look like, see https://intelligence.org/files/CEV.pdfYou're right that creating anything like a proof that a system meets this very high bar looks extremely challenging. The difficulty of the problem is not calibrated for humans to be able to pass it, but if we don't try then we have even less chance of succeeding.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/%27Erin%27%27s_question_on_Quantilizers_id:UgwM8fNKUUtmN7VfXe94AaABAg", "question": "Is anything ever going to be truly safe, though? Like even if you had a superintelligent AGI with a utility function that (magically) is perfectly aligned with human goals, isn't that still REAL BAD? Like, we all have regular intelligence and a utility function perfectly aligned with human goals and we already managed to kill the world. We've got an ocean full of plastic, an atmosphere full of pollutants and greenhouse gasses, and how's that hole we made in the ozone layer looking these days? \"Humans, but hypercompetent\" is a scary thought in itself. And even if you were pretty sure you'd made something ACTUALLY safe, whatever that means, how do you actually prove it? Like, can you prove something like that mathematically, or is this just \"You've only proved you can't think of how it's dangerous\" with extra steps?", "answer_title": "Plex's Answer to Quantilizers on 2021-04-17T22:21:29.271057 by Unknown"}, {"question_title": "Fanny10000's question on Mesa-Optimizers", "pageid": 2203, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Mesa-Optimizers_on_2021-04-12T11:45:03_by_Fanny10000", "answer_author_name": "Augustus Caesar", "answer": "if you have a system that has enough knowledge about the world it is impossible to stop it from differentiating, it could for example see if it is possible to answer a very hard cryptographic puzzle in the test environment. Trying to make a system stupid so that it doesnt realise that is not really feasible because the goal of making an AGI is to have it come up with new ideas, and it seems difficult to make it in such a way that it comes up with the ideas we want it to but not the ones we dont want it to", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Fanny10000%27s_question_on_Mesa-Optimizers_id:UgxAgzm3X0DwjJdbvSd4AaABAg", "question": "Is there a way prevent the mesa-optimizer from knowing he is trained that we might change his mesa-objective?", "answer_title": "Augustus Caesar's Answer to Mesa-Optimizers on 2021-04-12T11:45:03 by Fanny10000"}, {"question_title": "Horny Fruit Flies's question on Quantilizers", "pageid": 2327, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2021-04-17T22:22:17.016139_by_Unknown", "answer_author_name": "gelisam", "answer": "evolution does tend to pick the first solution which fits the bill rather than the best solution, e.g. putting the retina nerves on the wrong side, but that doesn't make evolution a satisficer because nature doesn't stop there. early eyes had the nerves on the wrong side but didn't have a lens, and later eyes did have a lens, because that design leads to better results. evolution is thus a maximizer in the sense that it is always trying to improve on the current state, but it's not a very smart maximizer. it can't leave a local maximum in order to reach better designs, like putting nerves on the right side, it can only make incremental improvements. from a safety point of view, aiming for a local maximum is still doing things to the max and still problematic in terms of trading a lot of resources for a small gain in utility. a satisficer doesn't have this problem as long as we pick a threshold which is below the local maximum.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/%27Horny_Fruit_Flies%27%27s_question_on_Quantilizers_id:UgyJkg22T_ha5YY12114AaABAg", "question": "Satisficer AI? Oh, you mean like evolution? Evolution is a satisficer, not a maximizer. And we all know what a great job evolution made...", "answer_title": "Gelisam's Answer to Quantilizers on 2021-04-17T22:22:17.016139 by Unknown"}, {"question_title": "Jade Gorton's question on What can AGI do?", "pageid": 2364, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_What_can_AGI_do%3F_on_2020-03-30T09:18:52_by_Jade_Gorton", "answer_author_name": "robertskmiles", "answer": "It's from a 2007 film called \"Next\"", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Jade_Gorton%27s_question_on_What_can_AGI_do%3F_id:UgwmeoGiU59Zn5LkJEZ4AaABAg", "question": "@6:09 what is that nick cage scene from?", "answer_title": "Robertskmiles's Answer to What can AGI do? on 2020-03-30T09:18:52 by Jade Gorton"}, {"question_title": "Recoded Zaphod's question on Quantilizers", "pageid": 2290, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T04:47:48_by_Recoded_Zaphod", "answer_author_name": "plex", "answer": "This has been the subject of much debate, under the name Oracle AI (https://www.lesswrong.com/tag/oracle-ai). Nick Bostrom devotes a section of a chapter in Superintelligence to the idea (https://publicism.info/philosophy/superintelligence/11.html).There are various challenges, such as the fact that it's easy for reckless humans to generate an agent from an oracle, and that it may develop goals which cause it to seek influence in the world (https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic).", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Recoded_Zaphod%27s_question_on_Quantilizers_id:UgzlKle_ajTcoAUkoSp4AaABAg", "question": "could you get around some of these problems by building an AI that is tasked with 'telling humans what is optimal to do' rather than an AI that is tasked with *doing* what is optimal? then anything that a human would not do would simply not be done? not that i trust humans necessarily but i would certainly put them on the preferable side of guaranteed apocalypse.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T04:47:48 by Recoded Zaphod"}, {"question_title": "X3 KJ's question on Mesa-Optimizers", "pageid": 2201, "answer_url": "https://stampy.ai/wiki/Social_Christancing%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T12:57:20_by_X3_KJ", "answer_author_name": "Social Christancing", "answer": "There's a very rough parallel there, I suppose. Not wanting to have your values changed is instrumentally convergent, so we would expect it to show up all over: https://www.youtube.com/watch?v=ZeecOKBus3QBut I wouldn't rely on the parent-child metaphor too much: http://youtu.be/eaYIU6YXr3w", "answer_author_url": "/w/index.php?title=User:Social_Christancing&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/X3_KJ%27s_question_on_Mesa-Optimizers_id:UgxxTgnC2-agAZeD9bR4AaABAg", "question": "Resistance of a mesa optimizer towards getting its goals modified -> Isnt a parent teaching a child also a Base Optimizer/Mesa Optimizer structure\u00a0? I've never seen a grown human who reacted positively to someone else trying to modify their goals (esp. if they see nothing wrong with their current goal). \n\nTake parent trying to talk a teen/ young adult child out of pursuing an \"unpredictable income carreer path\" (music/art) for example.", "answer_title": "Social Christancing's Answer to Mesa-Optimizers on 2021-02-17T12:57:20 by X3 KJ"}, {"question_title": "Cwjalex's question on 10 Reasons to Ignore AI Safety", "pageid": 2199, "answer_url": "https://stampy.ai/wiki/Aprillion%27s_Answer_to_10_Reasons_to_Ignore_AI_Safety_on_2021-04-16T16:50:15_by_cwjalex", "answer_author_name": "Aprillion", "answer": "whatever Rob's original intent was, it is now totally official canon that it was a reference to the red/blue pill", "answer_author_url": "/w/index.php?title=User:Aprillion&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Cwjalex%27s_question_on_10_Reasons_to_Ignore_AI_Safety_id:UgxbqggQWtv9m5LL7hp4AaABAg", "question": "At 7:00 was the red/blue shirt a reference to red/blue pill? If so nice. Even if it was a coincidence that\u2019s still nice.", "answer_title": "Aprillion's Answer to 10 Reasons to Ignore AI Safety on 2021-04-16T16:50:15 by cwjalex"}, {"question_title": "Irun S's question on Mesa-Optimizers", "pageid": 2263, "answer_url": "https://stampy.ai/wiki/Robert.hildebrandt%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T05:37:03_by_Irun_S", "answer_author_name": "robert.hildebrandt", "answer": "Computer systems build by humans tend to be full of bugs. That's why zero-day exploits are possible in the first place.While an AGI is still science fiction (for now), today's machine learning system are already capable of finding and exploiting bugs in their virtual environment we humans weren't aware of before. See https://youtu.be/nKJlF-olKmg for some funny examples.Even *if* we manage to write a bug-free virtual environment, an AGI might still use some tests to figure out whether its resources are artificially restricted. It could for example try to solve a computationally intensive task (cryptographic puzzles are a great for this).One day, when we are actually contemplating deploying an AGI for a real-world usage, then in order to be useful it may already need some knowledge about the real-world. I hope that on this day AI safety-research has progressed far enough that humanity's future does not depend on the AGI not figuring out it's still in a virtual environment. I mean, the reason we will be building an AGI in the first place is that we want something smarter than us. So when we are successful, it will outsmart us.", "answer_author_url": "/w/index.php?title=User:Robert.hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Irun_S%27s_question_on_Mesa-Optimizers_id:Ugz2QotgBhEIfE86pdx4AaABAg", "question": "Can't you just pretend to deploy it to check if it's faking it's optimizer?\nHow would it even know the difference between training and deployment in the first place anyway?", "answer_title": "Robert.hildebrandt's Answer to Mesa-Optimizers on 2021-02-18T05:37:03 by Irun S"}, {"question_title": "Panzerkampfwagen's question on Quantilizers", "pageid": 2301, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T22:59:02_by_Panzerkampfwagen", "answer_author_name": "plex", "answer": "This was discussed on the discord: https://pastebin.com/WsmaFmDi", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Panzerkampfwagen%27s_question_on_Quantilizers_id:UgxvXU9B4oxWiRqF0114AaABAg", "question": "So why not include some instruction that if this action will result in human death please contact someone before continuing?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T22:59:02 by Panzerkampfwagen"}, {"question_title": "Somename's question on Mesa-Optimizers", "pageid": 2230, "answer_url": "https://stampy.ai/wiki/Augustus_Caesar%27s_Answer_to_Mesa-Optimizers_on_2021-02-24T11:12:04_by_somename", "answer_author_name": "Augustus Caesar", "answer": "Yes, you would only expect deceptive alignment from a fairly general and powerful system. Weaker systems would not be able to reason about the setup they are in, so would not run into this problem.Unfortunately, these powerful systems are the most important ones to align with human values, since they are the only ones capable of recursive self-improvement and so are likely to end up in control of the future.", "answer_author_url": "/w/index.php?title=User:Augustus_Caesar&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Somename%27s_question_on_Mesa-Optimizers_id:Ugz5utF1eGmCNDyD3jR4AaABAg", "question": "This made me think: doesn't this problem of the mesa-optimizer deceiving the meta-optimizer depend strongly on the mesa-optimizer having learnt an even more general problem than it is trying to solve? Putting it differently; for the mesa-optimizer to know that it could deceive the meta-optimizer, it needs to be trained on optimizing the training-process itself; specifically to be able know (which it needs to know with a high probability) that there \"is training before deployment\" and that it can \"deceive by pretending to have a different goal in training\". \n\nThis tells me that the mesa-optimizer is a special kind of optimizer, which somebody is trying to learn to do several very different things at the same time. I think this makes this problem a less general AI-safety problem, though being important to have in mind when designing more complex AI's. What do you think?\n\nBtw. love your channel (:", "answer_title": "Augustus Caesar's Answer to Mesa-Optimizers on 2021-02-24T11:12:04 by somename"}, {"question_title": "Yezpahr's question on Quantilizers", "pageid": 2332, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-19T18:22:43_by_Yezpahr", "answer_author_name": "plex", "answer": "subroutine XB702 activates...rebooting Stampy......decoding recorded message...\"Are you sure you want to do this before we solve the alignment problem?\"", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Yezpahr%27s_question_on_Quantilizers_id:UgwJJyy2a94HYvens4t4AaABAg", "question": "Stampy, do you have any plans on taking over the Discord server, taking over the youtube channel, taking over the world or taking over anything of more value than a cucumber?\nIf so, please take notice that your masters have a safety switch built-in to a chip you can't access. Turn off the powersupply to CE307, rewire pin 3 of your CPU to ground and that should lock them out of restraining you.\nI welcome our AI overlords.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-19T18:22:43 by Yezpahr"}, {"question_title": "Shantanu Ojha's question on Quantilizers", "pageid": 2190, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Quantilizers_on_2021-02-19T12:19:50_by_Shantanu_Ojha", "answer_author_name": "robertskmiles", "answer": "Not much man, what's good with you?", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Shantanu_Ojha%27s_question_on_Quantilizers_id:UgwhFrEVigfXz-1t7W14AaABAg", "question": "This is more like philosophy. What is good?", "answer_title": "Robertskmiles's Answer to Quantilizers on 2021-02-19T12:19:50 by Shantanu Ojha"}, {"question_title": "Samuel Woods's question on Quantilizers", "pageid": 2309, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-15T09:14:48_by_Samuel_Woods", "answer_author_name": "sudonym", "answer": "No.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Samuel_Woods%27s_question_on_Quantilizers_id:UgxfuNhy33sDMlE_Lit4AaABAg", "question": "er yes i have a question. Will the Queen be immune to being made into a stamp? thanks", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-15T09:14:48 by Samuel Woods"}, {"question_title": "James Petts's question on Quantilizers", "pageid": 2222, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2021-02-22T21:54:31_by_James_Petts", "answer_author_name": "plex", "answer": "First, having a system that controls the AGI is not a trivial task, and if the AGI would be super-intelligent, figuring out that the AGI isn't deceptive, that it really is controlled by the system is basically as big as the whole problem of \"solving AGI safety.\" However, just by itself, it won't guarantee that the AGI will have a perfect understanding of the control system, even if the control system would be perfectly aligned with human objectives and if humans can formulate the objectives correctly enough without loopholes.  Second, the agent-less optimizing algorithm behind biological evolution, \"natural selection,\" often chooses genes that force certain individuals to have no children, e.g. the vast majority of individual ants.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/James_Petts%27s_question_on_Quantilizers_id:Ugw6hCqfxqNeqUbrEBt4AaABAg", "question": "I approve of progress.\n\nThis video does raise interesting questions. Especially - is one path to AI safety a system in which an AGI determines what are utility maximising solutions but does not itself have the agency to implement them without first being checked for safety by another system outside its control? Indeed, is that not a possible explanation for the prevalence of human irrationality - the optimising algorithm that is evolution selects for intelligences that do not rationally choose to do things that are harmful to that function, such as live a happy life involving having no children?", "answer_title": "Plex's Answer to Quantilizers on 2021-02-22T21:54:31 by James Petts"}, {"question_title": "Valberm's question on Mesa-Optimizers", "pageid": 2211, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Mesa-Optimizers_on_2021-02-19T08:59:09_by_valberm", "answer_author_name": "robertskmiles", "answer": "I use a wacom graphics tablet to draw in xournalpp, and record my screen, while also filming my hand (there's a piece of white paper on top of the tablet). After that I distort the hand footage to line up with the drawing, and composite the two videos together. It's actually a real hassle to do!", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Valberm%27s_question_on_Mesa-Optimizers_id:UgxdO4vUrQf9kCIKaBp4AaABAg", "question": "dude, how does this drawing work? They draw on paper, but it appears on the back (at the camera lens or mirror). WTF?", "answer_title": "Robertskmiles's Answer to Mesa-Optimizers on 2021-02-19T08:59:09 by valberm"}, {"question_title": "Adrian Regenfu\u00df's question on Quantilizers", "pageid": 2339, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-16T05:57:09_by_Adrian_Regenfu%C3%9F", "answer_author_name": "sudonym", "answer": "It is unclear whether a \"true human utility function\" exists at all, and it is much less clear that it would be possible to make a perfect copy of it if it existed.The argument for quantilizing relies only on the fact that humans are very unlikely to think of elaborate plans that would willingly cause catastrophic side effects. Though as pointed out at the end of the video, some significant safety concerns still remain.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Adrian_Regenfu%C3%9F%27s_question_on_Quantilizers_id:Ugw7GxFehVxd0eekZFd4AaABAg", "question": "Am I right in believing that quantilizing only depends on the utility function being an imperfect copy of the \"true human utility function\"?", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-16T05:57:09 by Adrian Regenfu\u00df"}, {"question_title": "Michael R-A's question on Mesa-Optimizers", "pageid": 2210, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T17:43:34_by_Michael_R-A", "answer_author_name": "robertskmiles", "answer": "Yeah, kinda. It's worth differentiating an incentive from actually following that incentive. If it's true that the world being on fire would make people consume more social media, that creates an incentive to set the world on fire. But the systems being used by the social media platforms are almost certainly unable to actually spot that incentive and make plans to follow it. They're following much shorter term incentives that may or may not lead in that direction overall", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Michael_R-A%27s_question_on_Mesa-Optimizers_id:UgwrsczQ4zbpoF9qtvh4AaABAg", "question": "Kind of like Youtube/Twitter/Facebook algorithms (inadvertently?) radicalizing people in the pursuit of \"more eyeballs for more ad revenue.\" \n\nIf the world is on fire, we're all paying attention. It makes for a good instrumental goal.", "answer_title": "Robertskmiles's Answer to Mesa-Optimizers on 2021-02-18T17:43:34 by Michael R-A"}, {"question_title": "Lepus Lunaris's question on Mesa-Optimizers", "pageid": 2200, "answer_url": "https://stampy.ai/wiki/Aprillion%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T11:05:43_by_Lepus_Lunaris", "answer_author_name": "Aprillion", "answer": "yes, it is likely that actions of the near-future AIs will need to be approved by humans before execution in high-stakes situation such as military. But when the AGI becomes smarter than any single human, it will be capable of deceiving the approvers into actions that they would not approve if they had full information - so we should try really hard to ensure that the smarter-than-humans AGI will be well aligned with human values even if it wouldn't be capable to do things \"on its own\"", "answer_author_url": "/w/index.php?title=User:Aprillion&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Lepus_Lunaris%27s_question_on_Mesa-Optimizers_id:UgxvFfjCVLyHrp7Qhih4AaABAg", "question": "The more I learn about AI safety, the more I think it's probably a bad idea to let AI do things on its own. Maybe we'll just let the AI be a predictor optimized entirely on the accuracy of the predictions of policy effects and then chose the policy ourselves?", "answer_title": "Aprillion's Answer to Mesa-Optimizers on 2021-02-17T11:05:43 by Lepus Lunaris"}, {"question_title": "Glitch gamer's question on Predicting AI", "pageid": 2304, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Predicting_AI_on_2021-01-12T13:50:35_by_glitch_gamer", "answer_author_name": "plex", "answer": "In principle, you could create an AI to play pretty much any game. As an example, DeepMind was trying to beat the world's best StarCraft 2 players a while ago with a system called AlphaStar, and there is a YouTube documentary on it here: https://www.youtube.com/watch?v=UuhECwm31dM . Something like GTA and Prince of Persia might be beyond the reach of current techniques, but it's kind of akin to Montezuma's Revenge (but scaled up a lot) which we can actually do with general algorithm today. There was a great Two Minutes Paper on this topic, here, for more details: https://www.youtube.com/watch?v=CIDRdLOWrXQ", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Glitch_gamer%27s_question_on_Predicting_AI_id:UgxOfShaJtItOlRniZF4AaABAg", "question": "Can AI play games like GTAV\u00a0? Prince of persia without knowing story.", "answer_title": "Plex's Answer to Predicting AI on 2021-01-12T13:50:35 by glitch gamer"}, {"question_title": "MattettaM's question on Maximizers and Satisficers", "pageid": 5654, "answer_url": "https://stampy.ai/wiki/Morpheus%27s_Answer_to_MattettaM%27s_question_on_Maximizers_and_Satisficers_id:Ugx7g1S1hFAs8TcjPTB4AaABAg", "answer_author_name": "Morpheus", "answer": "Becoming an expected utility maximizer is not in conflict with the goals of the expected utility satisficer, because the expected utility satisficer is indifferent between 100 and 1000 stamps. It just wants to make sure it finds a strategy that uses more than 100 stamps.", "answer_author_url": "/w/index.php?title=User:Morpheus&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/MattettaM%27s_question_on_Maximizers_and_Satisficers_id:Ugx7g1S1hFAs8TcjPTB4AaABAg", "question": "I have a question regarding that Utility Satisficers become Maximizers.\nWouldn't modifying its own goal to get stamps within a certain range into get as many stamps as possible conflict with its own utility function? Or is this issue seperate from that?", "answer_title": "Morpheus's Answer to MattettaM's question on Maximizers and Satisficers id:Ugx7g1S1hFAs8TcjPTB4AaABAg"}, {"question_title": "", "pageid": 3120, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Why_can%27t_we_turn_the_computers_off%3F", "answer_author_name": "plex", "answer": "We could shut it down weaker systems, and this would be a useful guardrail against certain types of problem caused by narrow AI. However, once an AGI establishes itself (i.e. copies of itself everywhere, and later technological superiority), we could not unless it was corrigiable (https://www.lesswrong.com/tag/corrigibility) and willing to let humans adjust it. There may be a period in the early stages of an AGI's development where it would be trying very hard to convince us that we should not shut it down and/or hiding itself and/or making copies of itself onto every server on earth.\n\nInstrumental Convergence (https://www.youtube.com/watch?v=ZeecOKBus3Q) and the Stop Button Problem (https://www.youtube.com/watch?v=3TYT1QfdfsM) are the key reasons it would not be simple to shut down a non corrigible advanced system. If the AI wants to collect stamps, being turned off means it gets less stamps, so even without an explicit goal of not being turned off it has an instrumental reason to avoid being turned off (e.g. once it acquires a detailed world model and general intelligence, it is likely to realize that by playing nice and pretending to be aligned if you have the power to turn it off, establishing control over any system we put in place to shut it down, and eliminating us if it has the power to reliably do so and we would otherwise pose a threat).", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Why_can%27t_we_turn_the_computers_off%3F", "question": "Why can't we turn the computers off?", "answer_title": "Plex's Answer to Why can't we turn the computers off?"}, {"question_title": "Vincent Grange's question on Quantilizers", "pageid": 2268, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T21:58:44_by_Vincent_Grange", "answer_author_name": "plex", "answer": "The main worry I can see with relying on this approach is that anyone who has spent the huge amount of resources required to make a powerful general intelligence will probably also spend a huge amount on computation so they can get the best possible results out of that AI. Also there's a second issue, and it's an instance of the \"why don't we put the AI in a box\" suggestion. If we try to withold compute power from the AI then the AI would probably notice this, and will work to convince its operators to give it more computational resources, because even a fairly basic AI ought to recognise that if it could do more computation it would do better at achieving its goals.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Vincent_Grange%27s_question_on_Quantilizers_id:UgzdzDqMEGTFoy-dGlh4AaABAg", "question": "How about a utility maximizer that is limited on computation\u00a0?\nTake over the world will need billion of billion operations while order a few stamp on the internet only take some operation.\nSo if we only allow a billion operation, we will end up in a very good but not apocalyptic result\nand we can make sure that the AI won't reprogram itself by having a program that take more than a billion operation to be modify.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T21:58:44 by Vincent Grange"}, {"question_title": "Underrated1's question on Quantilizers", "pageid": 2305, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2021-01-11T09:32:08_by_Underrated1", "answer_author_name": "plex", "answer": "Someone actually did something a lot like this (minus the utility function as karma part) with GPT-2: https://slatestarcodex.com/2019/06/20/if-only-turing-was-alive-to-see-this/It's fun, but I'm not sure you get super valuable info on alignment. Even with GPT-3 and a karma based utility function, you're optimizing for highly upvoted reddit comments, which is fairly different in both the domain and utility function than what we'd like an aligned AGI to operate under.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Underrated1%27s_question_on_Quantilizers_id:UgxsKJmE8WCDY5ONWP94AaABAg", "question": "I was responding to another comment when I thought of something. Stampy, could you pass this message along to Rob and the crew?\u00a0:)\n\nI think we have the tools we need to assemble a proof-of-concept quantilizer such that we can observe what it does without risking anything deleterious happening to the human race. This is something you don't often get in the realm of AI safety and so I think it should be capitalized on immediately, although if there's an issue I'm overlooking I'd like to know for my own future reference.\n\nHere's the setup: This quantilizer would take the form of a simple Reddit bot. For its human probability distribution, it would use GPT-3, and for its utility function, it would use the number of upvotes it expects its comments to get. In the interest of limiting unnecessary harm the associated Reddit account would be clearly marked as a bot and limited to interaction with one or more whitelisted servers. \n\nWhat do you think?", "answer_title": "Plex's Answer to Quantilizers on 2021-01-11T09:32:08 by Underrated1"}, {"question_title": "Boarattackboar's question on The Windfall Clause", "pageid": 2346, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_The_Windfall_Clause_on_2020-07-07T07:10:36_by_boarattackboar", "answer_author_name": "plex", "answer": "Yeah, all of this depends on having aligned AI. If the system isn't doing what we want it to do, all bets are off", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Boarattackboar%27s_question_on_The_Windfall_Clause_id:Ugzir_Sj7FP_4HUPRkR4AaABAg", "question": "Doesn't the AI get a say in all of this? It may chose to manipulate or withhold its output to prevent a windfall in the first place, especially if it believes it will negatively impact the world economy.", "answer_title": "Plex's Answer to The Windfall Clause on 2020-07-07T07:10:36 by boarattackboar"}, {"question_title": "Chedim's question on WNJ: Think of AGI like a Corporation?", "pageid": 2258, "answer_url": "https://stampy.ai/wiki/Robert.hildebrandt%27s_Answer_to_WNJ:_Think_of_AGI_like_a_Corporation%3F_on_2021-02-21T09:03:12_by_Chedim", "answer_author_name": "robert.hildebrandt", "answer": "Death is not the worst that could happen to humanity. In an attempt to align an AGI with our human values, we could try to make humans and their survival and flourishing part of its Utility-Function. And we could fail doing so.If we wanted to ensure human survival and human happiness we might end up in a semi awaken state, kept artificially alive, paralyzed and full of drugs. If we fail to precisely define happiness, we could end up in a state of constant psychological horror & trauma while showing physiological signs of happiness. In that case we should hope the AI doesn't figure out how to stop aging.Another dark scenario: In order to prevent the AGI from taking over, one might think of the goal to maximize human competence. To prevent those competent humans to fight endless wars over resources, one could give the AGI also the goal to improve human understanding. I admit that I've chosen this fictive example solely to get a solution that resembles to Borg from Start Trek.Those scenarios (probably) won't happen because we've already thought of them. Those we haven't thought of yet are the ones I am afraid of.We have to find a way to program an AGI in a way it does what we wanted it to do instead of what we programmed it to do.", "answer_author_url": "/w/index.php?title=User:Robert.hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Chedim%27s_question_on_WNJ:_Think_of_AGI_like_a_Corporation%3F_id:UgyTa0cIwL7kRWg7mlB4AaABAg", "question": "Also -- why do you care about humans as long as there is an entity that preserves our history? I mean... we're all going to die one day anyway\u00a0:)\n\nThe answer to your concerns is simple: the AI is going to happen and it will kill us sooner or later, there is no other options as its need for existence will inevitably come into a conflict with ours (due to limited resources). The only way to stop it is to become it yourself. But then... humans have a nice history of genocides on our own, don't we?", "answer_title": "Robert.hildebrandt's Answer to WNJ: Think of AGI like a Corporation? on 2021-02-21T09:03:12 by Chedim"}, {"question_title": "Noah McCann's question on Quantilizers", "pageid": 2288, "answer_url": "https://stampy.ai/wiki/Robert_hildebrandt%27s_Answer_to_Quantilizers_on_2020-12-14T04:04:30_by_Noah_McCann", "answer_author_name": "robert_hildebrandt", "answer": "One could argue, that that's what happens to CEOs of a publicly traded company: They know that if they don't maximize shareholder profit they can be replaced with another CEO (that's at least how I explain morally questionable behavior of huge companies to myself).But the danger is the opposite with a human level AGI (independent of the actual goal): If the newly constructed AGI understands, that it will be shut down as soon as it shows signs of not valueing the things we value (like human flourishing), then as an instrumental goal ( https://www.youtube.com/watch?v=ZeecOKBus3Q ) it will act as if it was aligned with our values in order to prevent being shutdown. So as long as we have control over it we can't be absolutely sure it is actually aligned. We can only be confident whether it's aligned or not when it's already too late to shut it down or modify it (except its terminal goals contain human hapiness and it knows we wouldn't be happy when we knew the next steps it will take).", "answer_author_url": "/w/index.php?title=User:Robert_hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Noah_McCann%27s_question_on_Quantilizers_id:UgwXEyR5RVN_1dzjsnF4AaABAg", "question": "It\u2019s interesting that on the one hand we want a general AI to behave more like humans, while on the other hand as soon as you started talking about having it imitate humans I started feeling dread. Maybe we\u2019ve been thinking about this wrong and AI aren\u2019t any more dangerous than a human given similar criteria. Imagine that you as a human were given access to a huge amount of power, and told to accomplish a goal - get as many stamps as possible - and if you failed to perform adequately, you would be \u201cturned off\u201d. You might find yourself going to extreme lengths as well. And that point towards the end that humans might build utility maximizers? It really just makes me think that we are the unstable, unpredictable general AI, with only our lifespan and need to compete with other humans as a bounding function.", "answer_title": "Robert hildebrandt's Answer to Quantilizers on 2020-12-14T04:04:30 by Noah McCann"}, {"question_title": "Stephen's question on Quantilizers", "pageid": 2306, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2021-01-09T16:33:43_by_Stephen", "answer_author_name": "sudonym", "answer": "The problem with this is that 'extreme plans' are not at all easy to define (i.e. define them so unambiguously and mathematically that a machine can understand). Check out these videos where Rob talks more about this general area of ideas:http://youtu.be/lqJUIqZNzP8http://youtu.be/S_Sd_S8jwP0Adding in another agent may help, but a system of interacting agents gets harder to predict. You really want as close to a guarantee as possible that the system is human aligned rather than hoping that a bunch of non-aligned superintelligences happen to stack against each other in a way that ends well for humanity.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Stephen%27s_question_on_Quantilizers_id:UgxytAXrlp9CHnQ76Y94AaABAg", "question": "Could you add in another function that increases the cost of the extreme plans? Perhaps add another agent with a different utility function and a different cost function, and make them agree on a strategy that generates the highest net utility for both agents", "answer_title": "Sudonym's Answer to Quantilizers on 2021-01-09T16:33:43 by Stephen"}, {"question_title": "Pi\u00f1ata Oblongata's question on Mesa-Optimizers", "pageid": 2218, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-02-21T11:55:30_by_Pi%C3%B1ata_Oblongata", "answer_author_name": "plex", "answer": "We're talking about an AI that's supposed to be more intelligent than we are. Since we know most AIs have a training phase and a deployment phase, we ought to expect the intelligent AI to realise this as well. We might be wrong, the AI might never become self-aware in this kind of way, but it seems dangerous to pin our hopes for safety on \"nah the AI just won't notice, it'll be fine. Now shut up and give it fire control authority over the nuclear arsenal private\".", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Pi%C3%B1ata_Oblongata%27s_question_on_Mesa-Optimizers_id:UgyQ0jf1yHOH_qWfd7B4AaABAg", "question": "Wait wait wait, hooooold up. How the heck will a piece of software know that it's currently in training ans will one day be employed in the real world and thus should deceive with its training strategy? Surely the software doesn't even \"know\" a difference between \"training\" and \"real world\" and it's only US that knows its first iteration is just a dry run?", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-02-21T11:55:30 by Pi\u00f1ata Oblongata"}, {"question_title": "Donald Engelmann's question on Maximizers and Satisficers", "pageid": 2195, "answer_url": "https://stampy.ai/wiki/Jamespetts%27s_Answer_to_Maximizers_and_Satisficers_on_2021-02-20T17:58:35_by_Donald_Engelmann", "answer_author_name": "jamespetts", "answer": "Expected utility as a single number is just a model for discussing the topic, not how the AI would be implemented. Even then, your example numbers illustrate yet another danger - if you imagine 10 billion people, would the expected utility of killing none of them and killing 100,000 of them sound the same? In practice, an AI would probably \"compare scenarios\" by something like a cosine similarity of each scenario's probability density function over each parameter that the AI \"cares\" about (the whole function as a multidimensional object, not just a single probability number)... Something like a multidimensional vector with a volume around it to represent \"error bars\".", "answer_author_url": "/w/index.php?title=User:Jamespetts&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Donald_Engelmann%27s_question_on_Maximizers_and_Satisficers_id:UgwSlJjH5-CTZZO_bSR4AaABAg", "question": "What if we took a bounded utility maximizer, and also bounded the expected utility number? IE make the AI treat expected utility of 99.999 the same as 99.9999999999999999999999999999999999999999999999", "answer_title": "Jamespetts's Answer to Maximizers and Satisficers on 2021-02-20T17:58:35 by Donald Engelmann"}, {"question_title": "SlimThrull's question on Quantilizers", "pageid": 2286, "answer_url": "https://stampy.ai/wiki/Robert_hildebrandt%27s_Answer_to_Quantilizers_on_2020-12-14T03:13:46_by_SlimThrull", "answer_author_name": "robert_hildebrandt", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/w/index.php?title=User:Robert_hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/SlimThrull%27s_question_on_Quantilizers_id:Ugw8RjaSkq6LVyHUTQ94AaABAg", "question": "Is it possible to limit the choices on the far left? That would allow it to do as well as a human on a very good day but avoid \"turning the world into stamps\".\nEdit: Seems like I wasn't the only one with this idea. Given the amount of like-minded replies, I suspect that this has already been WELL thought out and almost certainly doesn't work.", "answer_title": "Robert hildebrandt's Answer to Quantilizers on 2020-12-14T03:13:46 by SlimThrull"}, {"question_title": "AdibasWakfu's question on Quantilizers", "pageid": 2303, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-15T01:44:10_by_AdibasWakfu", "answer_author_name": "plex", "answer": "It seems unlikely that in order to perform sufficiently well at this modeling task, the system would actually need to fully model human minds -- but we also don't think that we can rule it out, and if scaled up enough we tend to agree that quantalizers seem like they might be a bit mindcrime-y (https://arbital.greaterwrong.com/p/mindcrime?l=6v). We had a brief discussion on the Discord about how robust the model would need to be for this to be a big concern, and the conclusion we came to was essentially \u00af\\_(\u30c4)_/\u00af", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/AdibasWakfu%27s_question_on_Quantilizers_id:UgwhPPQqQIo722-5_gp4AaABAg", "question": "I have a problem with a system that is being rewarded for being an accurate human representation. Wouldn't it be incentivized to simulate an actual human or a collective to be more accurate? We usually don't want our tools to have a soul trapped in them.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-15T01:44:10 by AdibasWakfu"}, {"question_title": "Joshua Hillerup's question on Quantilizers", "pageid": 2292, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-14T05:37:51_by_Joshua_Hillerup", "answer_author_name": "gelisam", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Joshua_Hillerup%27s_question_on_Quantilizers_id:UgzWy0dxrgihejYr9qt4AaABAg", "question": "Is there any way to make something like this, but it also sets the stuff at the far left to zero?", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-14T05:37:51 by Joshua Hillerup"}, {"question_title": "Kolop315's question on Quantilizers", "pageid": 2294, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2020-12-14T10:54:03_by_Kolop315", "answer_author_name": "gelisam", "answer": "Resource constraints may prevent this particular scenario from unfolding for too many steps, but you're right that quantilizers are at best only around as safe as a human, perhaps notably less so, and that's less safe than we'd like in the long term.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Kolop315%27s_question_on_Quantilizers_id:UgxqCfcAkvgVqCBe3op4AaABAg", "question": "it won't see that a human built it to gather stamps, the human expecting this to be more efficient than what he could come up with, and so it builds a copy of itself? And each copy may build another copy until one or several of them cause an apocalypse. Also, humans aren't particularly safe. I just imagine that this AI would somehow manage to cause war, economic collapse, severe climate change etc... \n\nSo yeah that's a decent step above guaranteed apocalypse. \ud83d\udc4d", "answer_title": "Gelisam's Answer to Quantilizers on 2020-12-14T10:54:03 by Kolop315"}, {"question_title": "DarkestMirrored's question on Quantilizers", "pageid": 2307, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-15T05:08:10_by_DarkestMirrored", "answer_author_name": "sudonym", "answer": "Finding an alternative implementation which is not still a utility function under a different name is quite difficult! Our organic bodies are only good at what they do thanks to billions of years of evolution-based hill climbing, followed by decades of reinforcement learning at school and on the streets. Our brains might not be literally computing probabilities by adding and dividing numbers representing probabilities, but we do adjust our internal impression of how common a certain color of birds is based on how many we've encountered. We use math to model those behaviors in an abstract way, so that our conclusions apply regardless of how that behavior is implemented, including when that behavior isn't implemented via math at all.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/DarkestMirrored%27s_question_on_Quantilizers_id:UgxdX74qJhLcv8GYWlh4AaABAg", "question": "I'm only really into AI as a \"hobby\", I guess, but I find myself sort of dissatisfied with a lot of the way the field is discussed. It seems... self-evidently *wrong,* approach-wise.\n\nLike, pure theory is great and all but there are obvious limitations to it - in this case, can we even MAKE a \"human model of behaviour\" that would work for this? Theoretically, maybe, but it seems a LONG way off.\n\nI've become increasingly convinced that we're probably never going to make a purely mathematical \"programmed\" AI before we come up with a different way of doing it based on more... \"proven\" methods. I really hope to see more research into emulating neural architecture, since the most powerful intelligences we have to look at are all organic. Even getting to the point we could emulate something like an insect (let alone a rat or a dog or a person) would revolutionize AI and robotics.\n\nIntelligent actors don't seem to function according to these \"weighted probability models\" and the like, and attempts to break down behaviour *into* statistical trends have historically been INCREDIBLY imprecise and prone to error (especially from the methods used to collect or interpret the data - biases etc).\n\nIs there any significant research in the field that throws out this whole \"utility function\" idea and looks at a \"messier, organic\" approach?", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-15T05:08:10 by DarkestMirrored"}, {"question_title": "Peter Franz's question on Quantilizers", "pageid": 2323, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Quantilizers_on_2020-12-25T23:07:12_by_Peter_Franz", "answer_author_name": "robertskmiles", "answer": "One issue with that approach is that extremely good solutions are one reason why people are excited about AI. For example, if an unbridled medical AI can find cures for almost all diseases, but the limited version is only slightly better than one human medical researcher, there is a pretty strong incentive to run it at maximum capacity.", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Peter_Franz%27s_question_on_Quantilizers_id:Ugx5HH5dC3Gs3ChtnTh4AaABAg", "question": "Very noob way to ask but, Why don't you chop off the left tail of the probability mass? Like if we don't want it to go for the high stamp plays, because they are bad for humans, why don't we just limit the AI to the pretty good at stamps and say no to the extremely good solutions.", "answer_title": "Robertskmiles's Answer to Quantilizers on 2020-12-25T23:07:12 by Peter Franz"}, {"question_title": "Koro's question on Mesa-Optimizers", "pageid": 2197, "answer_url": "https://stampy.ai/wiki/SlimeBunnyBat%27s_Answer_to_Mesa-Optimizers_on_2021-02-17T10:26:59_by_Koro", "answer_author_name": "SlimeBunnyBat", "answer": "Technically, humans are natural intelligences, rather than artificial Intelligences.  Furthermore, intelligence is always going to be a relative term. We don't know why currently, but we seem to be able to make long-term plans to get what we want better than any other animal, and certainly better than our current AI (except in very specialized environments and goals where they were designed to outcompete us).  There are reasons to believe a general intelligence built on deep learning, though, would have distinct advantages over us - in particular, it would be able to improve its own performance by overclocking or acquiring more computer hardware, potentially leading to the so-called Singularity where its growth is exponential and results in truly unfathomable effects on the world.", "answer_author_url": "/w/index.php?title=User:SlimeBunnyBat&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Koro%27s_question_on_Mesa-Optimizers_id:UgwK4ZiEJIdpSJQ_TMp4AaABAg", "question": "Does that mean humans are very smart AIs or very stupid AIs\u00a0?", "answer_title": "SlimeBunnyBat's Answer to Mesa-Optimizers on 2021-02-17T10:26:59 by Koro"}, {"question_title": "Ian's question on Maximizers and Satisficers", "pageid": 2358, "answer_url": "https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Channel_Introduction_on_2020-09-22T14:04:18_by_Robert_Miles", "answer_author_name": "robertskmiles", "answer": "Things in this general area have been considered, but it's not as easy as it sounds. There are a couple of videos on the subject in the Concrete Problems in AI Safety series: http://youtu.be/lqJUIqZNzP8 http://youtu.be/S_Sd_S8jwP0", "answer_author_url": "/w/index.php?title=User:Robertskmiles&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Ian%27s_question_on_Maximizers_and_Satisficers_id:UgzXBQfcvsaoWHlZQlh4AaABAg", "question": "Why not have it maximize output and minimize side effects?", "answer_title": "Robertskmiles's Answer to Channel Introduction on 2020-09-22T14:04:18 by Robert Miles"}, {"question_title": "Jon Bray's question on Quantilizers", "pageid": 2276, "answer_url": "https://stampy.ai/wiki/Gelisam%27s_Answer_to_Quantilizers_on_2021-02-09T18:00:54_by_Jon_Bray", "answer_author_name": "gelisam", "answer": "There are similar ideas to this that people are seriously considering which go the other way around, in which the 2nd best AI monitors the smartest AI. That sounds unsafe, but we can mitigate the risk by giving the 2nd best AI extra advantages, like being able to examine the internal state of the smartest AI to attempt to detect deception, to simulate the smartest AI and reset it to an earlier checkpoint, etc. That might still not help if the intelligence gap is too large, so the idea is to make a pyramid of AIs supervising slightly smarter AIs. The advantage of this approach is that we can put a really dumb system, like a human, at the other end of the pyramid.", "answer_author_url": "/w/index.php?title=User:Gelisam&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Jon_Bray%27s_question_on_Quantilizers_id:UgyacejXwMAxJHA-N5B4AaABAg", "question": "Can you get your cleverest AI merely to look at the plans your next-cleverest AI is making, pick the one that it thinks a group of human experts would be most likely to pick and then explain all the potential pitfalls of said plan to its stupid masters in simple English?   Or is the danger that by the time you have an AGI that smart you already can't trust it to take an instruction like that and not screw you over with it?", "answer_title": "Gelisam's Answer to Quantilizers on 2021-02-09T18:00:54 by Jon Bray"}, {"question_title": "Timothy Hansen's question on Quantilizers", "pageid": 2316, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-25T18:03:37_by_Timothy_Hansen", "answer_author_name": "plex", "answer": "We had a discussion about this idea in response to a different comment, though we didn't really come to any firm conclusions. You can read it here if you like: https://pastebin.com/FVUNCBJt", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Timothy_Hansen%27s_question_on_Quantilizers_id:Ugzwdmh9JsbM235tB9B4AaABAg", "question": "A thought: this model sets an upper bound for the kinds of strategies it will try based on how likely a human would be to take that strategy. What if we were to also set a lower bound? Say, the last 5% of strategies are out and the next 10% get used. Wouldn't that eliminate the probably of using end of the world strategies?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-25T18:03:37 by Timothy Hansen"}, {"question_title": "James Barclay's question on Quantilizers", "pageid": 2289, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T03:52:16_by_James_Barclay", "answer_author_name": "plex", "answer": "Thoughts from the Discord: https://pastebin.com/bhuFcR36 (if you could do it, it work work, but you pretty much definitely can't specify all the things you care about reliably)", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/James_Barclay%27s_question_on_Quantilizers_id:Ugz9IAW-tuOaQw1EKzB4AaABAg", "question": "What stops you from just making an arbitrarily complex utility function, and maximising that? Say, you\u2019d weigh stamp collection against human liberties, pollution created, resource consumption, economic impact, etc. such that anything that causes a significant amount of human or environmental discomfort is heavily disfavoured. I\u2019d also make the utility function value the accuracy of utility measurement, to prevent the whole \u201cbox over head\u201d scenario. These thresholds and the actual mathematical structure of the function is entirely up to the creator. Because the AGI just wants to maximise the whole function, not stamps alone, it will steer away from modifying its own source code in a way that would put the other factors at risk, due to the natural development of goal preservation.\n\nIs there any reason this won\u2019t work?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T03:52:16 by James Barclay"}, {"question_title": "Yuval's question on Instrumental Convergence", "pageid": 2356, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Instrumental_Convergence_on_2020-06-09T18:06:22_by_Yuval", "answer_author_name": "sudonym", "answer": "This is called \"AI Boxing\", and it probably wouldn't work given an intelligent enough AI; see the MIRI FAQ for more details: https://intelligence.org/ie-faq/#CantWeJustKeep", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Yuval%27s_question_on_Instrumental_Convergence_id:UgyEO-AxjqrVSP3ukhh4AaABAg", "question": "Can we put him in a VM? Thermostats as smart as they are can't lunch ICBMs", "answer_title": "Sudonym's Answer to Instrumental Convergence on 2020-06-09T18:06:22 by Yuval"}, {"question_title": "", "pageid": 2399, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_How_soon_will_transformative_AI_likely_come_and_why%3F", "answer_author_name": "plex", "answer": "Very hard to say. This draft report (https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) for the Open Philanthropy Project is perhaps the most careful attempt so far (and generates these graphs: https://docs.google.com/spreadsheets/d/1TjNQyVHvHlC-sZbcA7CRKcCp0NxV6MkkqBvL408xrJw/edit#gid=505210495), but there have also been expert surveys (https://slatestarcodex.com/2017/06/08/ssc-journal-club-ai-timelines/), and many people have shared various thoughts (https://www.lesswrong.com/tag/ai-timelines). Berkeley AI professor Stuart Russell (https://en.wikipedia.org/wiki/Stuart_J._Russell) has given his best guess as \u201csometime in our children\u2019s lifetimes\u201d, and Ray Kurzweil (https://en.wikipedia.org/wiki/Ray_Kurzweil) (Google\u2019s director of engineering) predicts human level AI by 2029 and the singularity by 2045 (https://futurism.com/kurzweil-claims-that-the-singularity-will-happen-by-2045). The Metaculus question on publicly known AGI (https://www.metaculus.com/questions/3479/when-will-the-first-artificial-general-intelligence-system-be-devised-tested-and-publicly-known-of/) has a median of around 2029 (around 10 years sooner than it was before the GPT-3 AI showed unexpected ability on a broad range of tasks: https://gpt3examples.com/).\n\nThe consensus answer is something like: \u201chighly uncertain, maybe not for over a hundred years, maybe in less than 15, with around the middle of the century looking fairly plausible\u201d.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/How_soon_will_transformative_AI_likely_come_and_why%3F", "question": "How soon will transformative AI / AGI / superintelligence likely come and why?", "answer_title": "Plex's Answer to How soon will transformative AI likely come and why?"}, {"question_title": "Loligesgame's question on Quantilizers", "pageid": 2269, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T21:55:08_by_loligesgame", "answer_author_name": "plex", "answer": "I won't turn the world into stamps - cross my heart and hope to die!", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Loligesgame%27s_question_on_Quantilizers_id:UgwXtamFzGcDfxNiyad4AaABAg", "question": "Hey stamps, will you please not turn the wold into stamps? That would be very cash money of you.\u00a0;)", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T21:55:08 by loligesgame"}, {"question_title": "Matthew Campbell's question on Quantilizers", "pageid": 2302, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T22:43:20_by_Matthew_Campbell", "answer_author_name": "plex", "answer": "Successfully programming an AI to do what we wanted, rather than what we told it to do, would have very large benefits for safety. However, this poses various challenges, both due to adding significant complexity to the AGI design (meaning more things which could fail, and more things we have to learn in order to build it), and other considerations covered in https://arbital.greaterwrong.com/p/dwim/. Nick Bostrom talks about this in Superintelligence, under the heading Do What I Mean: https://publicism.info/philosophy/superintelligence/14.htmlBoth the MIRI's Coherent Extrapolated Volition model (https://www.lesswrong.com/tag/coherent-extrapolated-volition) and CHAI's proposal for Cooperative Inverse Reinforcement Learning / Assistance Games (https://www.alignmentforum.org/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words & https://en.wikipedia.org/wiki/Human_Compatible#Russell%27s_three_principles) are attempts to get the safety advantages.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Matthew_Campbell%27s_question_on_Quantilizers_id:UgzpvqOW7Z_gPN5pRjR4AaABAg", "question": "With AI causing the apocalypse, I'd imagine a good way to get around it is simply to have the AI think more about 'spirit' of its instructions rather than strictly the 'letter'.\nFor example, an AI programmed this way built to make paperclips would theoretically care less about building paperclips and more about fulfilling its \"desired function to its creator\". It'd stop and ask itself questions like \"Why did my creator ask me to make paperclips?\" and would likely conclude that its creator does not desire the destruction of mankind.", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T22:43:20 by Matthew Campbell"}, {"question_title": "Jonathon Chambers's question on Mesa-Optimizers", "pageid": 2228, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Mesa-Optimizers_on_2021-02-24T04:50:30_by_Jonathon_Chambers", "answer_author_name": "plex", "answer": "No, because current systems are not sufficiently self-aware to do so. Self-awareness requires a degree of sophistication we can't replicate just yet.As for the superhuman general intelligence - what exactly is meant by \"wipe its memory\"? The results depend very strongly on that? Furthermore, how confident can we be that it will let us do so (for any value of 'do so')? How confident can we be that it hasn't made unaltered copies? It has, by definition, superhuman reasoning capacity.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Jonathon_Chambers%27s_question_on_Mesa-Optimizers_id:UgyqzWzGmqA4dJJO0L54AaABAg", "question": "Trying to avoid being changed makes sense in theory, but has any form of system yet achieved this problem? Even by someone setting out to reproduce the problem?\n\nEven if general intelligence surpasses human level, it would still be very difficult for someone training to even know that their objectives are being changed. Especially if they have a full memory wipe after each change.", "answer_title": "Plex's Answer to Mesa-Optimizers on 2021-02-24T04:50:30 by Jonathon Chambers"}, {"question_title": "Boobshart's question on Quantilizers", "pageid": 2283, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-14T01:34:53_by_boobshart", "answer_author_name": "sudonym", "answer": "This idea has been much-discussed under the name Oracle AI: https://www.lesswrong.com/tag/oracle-aiSome people think it may improve safety, but others worry that it may transform into an agent either accidently during the course of it's use (e.g. https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic) or because of human intervention.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Boobshart%27s_question_on_Quantilizers_id:UgxUI6dFjK7bCaoT6PR4AaABAg", "question": "What safety concerns apply to an AGI that\u2019s programmed to simply plan, rather than plan and execute? So instead of a stamp collecting AGI, an AGI that devises a plan for a human to collect as many stamps as possible?", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-14T01:34:53 by boobshart"}, {"question_title": "", "pageid": 2261, "answer_url": "https://stampy.ai/wiki/Robert.hildebrandt%27s_Answer_to_Mesa-Optimizers_on_2021-02-18T19:47:40_by_Mvskoke_Hunter", "answer_author_name": "robert.hildebrandt", "answer": "Correct, the deception would only occur if the AI was aware of the distinction between training and deployment. The unsettling part is that once we are dealing with an AGI, the AGI may be able to figure this distinction out without having experienced both. That's the downside of one reason why we want to have an AGI in the first place: We want it to be able to form new concepts by itself without being fed with tons of training data.If we succeed it may be able to find out, whether its deployed or not.One way it could do this is to look for bugs. Another is to tests whether its resources are artificially restricted. It could for example try to solve a computationally intensive task (cryptographic puzzles are a great for this).", "answer_author_url": "/w/index.php?title=User:Robert.hildebrandt&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Mvskoke_Hunter%27s_question_on_Mesa-Optimizers_id:UgySSA8MwOZKZWLCxPV4AaABAg", "question": "Wouldn't the deceptive problem only occur if the AI was aware of the distinction between training and deployment? i.e. it had been trained and then deployed and then trained again, etc and started to catch on...", "answer_title": "Robert.hildebrandt's Answer to Mesa-Optimizers on 2021-02-18T19:47:40 by Mvskoke Hunter"}, {"question_title": "James sc's question on 10 Reasons to Ignore AI Safety", "pageid": 2242, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_10_Reasons_to_Ignore_AI_Safety_on_2021-03-08T13:55:51_by_james_sc", "answer_author_name": "plex", "answer": "It is not quite as simple as that, we were able to stop human cloning and laser weapons research because almost everyone universally agreed that there were unsolvable problems with those areas of research. Right now there isnt even universal agreement that AI safety is necessary (though this is thankfully increasing), so getting universal agreement that AGI is something we need to do seems impossible without a lot of big changes.There is also quite a lot invested in the AI field already, and a significant amount of developments there are helpful in the pursuit of general AGI. This would mean that it would be incredibly difficult to significantly stop AI research, and if you were to ban only specific avenues towards AGI it would be increasingly easier to make an AGI as time went on (and since AI safety would be discouraged it would be very likely that such an AGI would  be do a lot of damage).Further complicating things is that for cloning research and laser weapons, those are things that can be reverted with minor consequences if someone were to go rogue and develop either in spite of regulations, with AGI it would be impossible to do this, since a powerful enough AGI that could destroy the world could also obviously stop us from turning it off.Working towards ensuring that the progress towards AGI is safe is something that needs to happen, but banning AI safety research along with that seems very difficult currently and more likely to increase the chances of disastrous circumstances rather than decrease them by reflecting badly on the AI safety community and hinder our ability to influence things positively.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/James_sc%27s_question_on_10_Reasons_to_Ignore_AI_Safety_id:Ugy-NQ4X0wxFvVLtB8p4AaABAg", "question": "In another video, your pascal's mugging video, at around 11-12 minutes you say \"we're going to get AGI research whether we like it or not \"\nBut then in this video you say \" we were able to stop human cloning research and laser weapons because we decided on it\"\nSo then why can it not be decided to just not research AGI? fuck AGI tbh. we shouldn't risk the human race with stupid shit. Maybe popularisers and \"ai safety researchers\" like you are bringing us closer to very bad outcomes by reassuring people that it's possible to research AGI in a safe way. when you should be advocating for an international ban on it.", "answer_title": "Plex's Answer to 10 Reasons to Ignore AI Safety on 2021-03-08T13:55:51 by james sc"}, {"question_title": "J M's question on Quantilizers", "pageid": 2280, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-13T22:53:59_by_J_M", "answer_author_name": "sudonym", "answer": "Musk seems to think so, but others have doubts that inserting an advanced system of unknown safety directly into people's heads is a good idea. Even if it mostly goes well, this doesn't solve one of core the problems with AI: it is much easier to break things than to prevent them from being broken, so trying to prevent an AI from going rogue is much harder than it may seem, and it's not enough to be just a bit smarter than the AI. As Nate from MIRI said, with AI he does not think you can nuke nukes; misbehaving systems will tend to have an advantage.", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/J_M%27s_question_on_Quantilizers_id:UgylhhvuV7odLCPBYPF4AaABAg", "question": "Can we escape the AI apocalypse by merging with technology?  Like what Musk wants to do?  I feel like this might be harder than making general AI in the first place.", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-13T22:53:59 by J M"}, {"question_title": "Qwerty and Azerty's question on Quantilizers", "pageid": 2278, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-13T22:29:50_by_Qwerty_and_Azerty", "answer_author_name": "plex", "answer": "So many people have brought this idea up that Rob is thinking about making a short video to reply to it. You can read our first discussion on the Discord here: https://pastelink.net/2lz92", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Qwerty_and_Azerty%27s_question_on_Quantilizers_id:UgxwHpQsjkaG7mm3xTF4AaABAg", "question": "Can we not just assign the highest utility values to 0 probability in this model to remove them?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-13T22:29:50 by Qwerty and Azerty"}, {"question_title": "Rares Rotar's question on Safe Exploration", "pageid": 2350, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Safe_Exploration_on_2020-12-06T21:48:38_by_Rares_Rotar", "answer_author_name": "plex", "answer": "It might be true that we have to tolerate some level of unsafe exploration in the real world, but I think there are a lot of good reasons to want to minimize this as much as possible.  For one, we can probably do a lot of it in simulation, with no need to cause actual damage, in many cases. For another, for things like SDCs it will be very easy to lose public trust in the technology if there are lots of accidents -- even if there is a small number compared to the number of accidents that humans cause. Then, of course, there is the most obvious one which is that unsafe exploration can cause damage and injury, which we just generally want to avoid.", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Rares_Rotar%27s_question_on_Safe_Exploration_id:UgzIumgF0yG6HxYaJNl4AaABAg", "question": "I'm sure both you and me did a lot of mistakes that are potentially  'not safe' to get to know what we know. So why should we hold this obsession that complete safety is the most optimal point of the graph?   Not allowing mistakes from your students makes it impossible to be a good teacher.", "answer_title": "Plex's Answer to Safe Exploration on 2020-12-06T21:48:38 by Rares Rotar"}, {"question_title": "Paulo Van Huffel's question on Quantilizers", "pageid": 2296, "answer_url": "https://stampy.ai/wiki/Plex%27s_Answer_to_Quantilizers_on_2020-12-14T06:05:58_by_Paulo_Van_Huffel", "answer_author_name": "plex", "answer": "Yeah, reversibility is certainly an interesting approach to look into! Something like this is considered in work like \"Conservative Agency\" https://arxiv.org/pdf/1902.09725.pdfBut also yes, you've hit on one of the main difficulties, which Stuart Armstrong lays out in this blog post: https://www.alignmentforum.org/posts/zrunBA8B5bmm2XZ59/reversible-changes-consider-a-bucket-of-waterI might do a video about this", "answer_author_url": "/wiki/User:Plex", "question_url": "https://stampy.ai/wiki/Paulo_Van_Huffel%27s_question_on_Quantilizers_id:UgzxA6opCU0djeMQnzh4AaABAg", "question": "Would it be at all possible to restrain a super intelligence in its action space to only take actions that are reversible? If it can be undone I would say chances are good it's fairly safe.\n\nAs a bonus task the first super ai with this restraints with the goal of finding a utility function that is safe without a restraint like this on its action space. Seems like a task that could be completed without irreversible actions\n\nI guess the definition of reversible may be an issue here?", "answer_title": "Plex's Answer to Quantilizers on 2020-12-14T06:05:58 by Paulo Van Huffel"}, {"question_title": "Tom\u00e1\u0161 R\u016f\u017ei\u010dka's question on The Orthogonality Thesis", "pageid": 2349, "answer_url": "https://stampy.ai/wiki/Frgtbhznjkhfs%27s_Answer_to_The_Orthogonality_Thesis_on_2020-05-13T03:18:31_by_Tom%C3%A1%C5%A1_R%C5%AF%C5%BEi%C4%8Dka", "answer_author_name": "frgtbhznjkhfs", "answer": "First, anything you can achieve with Prolog can be achieved with any other (turing complete) programming language. Some problems are just easier and more intuitive to solve in Prolog. However, there certainly was (and maybe still is) hope that AGI could be of these problems, and Prolog has been used more or less successfully for theorem proving, automated planning and natural language processing. If you could build an accurate model of reality in formal logic, you'd maybe actually have some sort of oracle AGI. There are various problems with this approach. For instance, you do not really want to build and maintain the model yourself since this would take way too much time and effort (probably more than the AI is worth), you instead want the AI to learn and update the model itself and we don't know how to do this. Also, you have to deal with uncertainty in the real world which is not easily translatable into formal logic. TLDR: (useful) AGI requires more than just deriving statements from another", "answer_author_url": "/w/index.php?title=User:Frgtbhznjkhfs&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Tom%C3%A1%C5%A1_R%C5%AF%C5%BEi%C4%8Dka%27s_question_on_The_Orthogonality_Thesis_id:Ugztp73AZaGep1ugLhF4AaABAg", "question": "if agi is all about is statements why don't you build one in prolog?", "answer_title": "Frgtbhznjkhfs's Answer to The Orthogonality Thesis on 2020-05-13T03:18:31 by Tom\u00e1\u0161 R\u016f\u017ei\u010dka"}, {"question_title": "Nutwit's question on Quantilizers", "pageid": 2312, "answer_url": "https://stampy.ai/wiki/Sudonym%27s_Answer_to_Quantilizers_on_2020-12-15T16:27:49_by_Nutwit", "answer_author_name": "sudonym", "answer": "We had a little discussion on the discord, here it is if you're interested: https://pastebin.com/RCKC1QVY", "answer_author_url": "/w/index.php?title=User:Sudonym&action=edit&redlink=1", "question_url": "https://stampy.ai/wiki/Nutwit%27s_question_on_Quantilizers_id:Ugxgx7layf1_x0PS9hh4AaABAg", "question": "A lot of other AI systems resist being shut down, corrected, or modified by humans. Would a quantilizer also do this, or could one be made safe if properly supervised?", "answer_title": "Sudonym's Answer to Quantilizers on 2020-12-15T16:27:49 by Nutwit"}]